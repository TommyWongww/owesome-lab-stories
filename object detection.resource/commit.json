{"compress":true,"commitItems":[["2e80bbe0-643f-4678-afa9-3269dc1e968e",1524970876707,"---\nstyle: plain\n---\n\nLorem ipsum\n===========\n\n[[toc]]\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus eu tempor dolor. Suspendisse potenti. Curabitur blandit leo sit amet velit iaculis cursus.\n\n## Donec ornare\n\nPhasellus et massa aliquam, ullamcorper neque eu, dignissim elit. Duis venenatis turpis est. Sed finibus eros eget tellus fringilla, in pulvinar ex lobortis. Praesent vel urna tellus. Aliquam accumsan augue magna, rutrum pulvinar erat luctus sed. Nunc malesuada erat sed nulla pretium maximus. Vivamus consequat convallis porta. Nam congue scelerisque congue.\n\n### Pellentesque viverra\n\nInterdum et malesuada fames ac ante ipsum primis in faucibus. Morbi sagittis aliquam orci, id elementum leo varius at. Suspendisse in hendrerit orci. Aliquam nec congue [^1] augue. In volutpat scelerisque congue. Nam ultricies justo dictum ligula pulvinar iaculis. Etiam vulputate nulla eget lectus fermentum accumsan. Sed in sem tincidunt, sollicitudin erat pharetra, bibendum sapien.\n\n| Maecenas | Molestie | Nisi |\n| ------ | ------ | ------: |\n| Et dolor | Congue ultricies. Phasellus viverra convallis urna | eu |\n| Tempor eros | Sagittis eget. Nunc aliquet dolor a velit euismod vestibulum. Integer libero massa, pharetra sodales lacinia | et |\n| Faucibus | Ornare purus. Etiam vestibulum finibus | urna |\n| Quis | Pulvinar elit luctus sed. Duis vulputate vehicula magna ut congue. Nullam gravida orci ut est placerat, quis vulputate dui ornare. Suspendisse nec sapien id odio dignissim tempor. Aliquam at feugiat purus. Vivamus a euismod urna, in tempor | leo |\n\nSit amet scelerisque nisi interdum. Nulla porttitor [^2], libero sed tempus vulputate, velit nisl suscipit urna, quis dignissim nibh dui sed mi.\n\n### Praesent varius diam\n\nNam id imperdiet turpis. Fusce dignissim vel eros sit amet auctor. Donec quis lorem egestas, placerat odio vel, pulvinar tellus. Nullam tristique neque urna, non dapibus augue pulvinar et. Sed porta ac nulla in ultricies. Aenean ut mollis eros. Aliquam pellentesque vehicula sapien, vel finibus mi efficitur vitae. Donec vulputate aliquet felis sit amet semper. Cras eget odio vel metus bibendum egestas. Pellentesque a turpis sed ipsum tempus condimentum nec bibendum risus.\n\n* * *\n\n## Libero finibus facilisis ac a lacus\n\nInteger tincidunt nunc quis tortor tristique, ut egestas metus volutpat. In sit amet dolor leo. dictum in odio eu vulputate. Nullam a ornare elit, a sollicitudin augue.\n\n * Donec neque velit, gravida eu tempor nec, sagittis ut lectus. Quisque laoreet dignissim facilisis.\n\n * Cras vel rutrum odio, in mollis elit. Maecenas malesuada est sit amet eleifend vulputate. Quisque in ultrices erat.\n\n * Aenean mollis nibh et ligula accumsan semper vitae sed nisi. Ut pulvinar aliquam nulla, id cursus augue tincidunt sed.\n\n### Nullam at erat faucibus \n\nMi interdum sodales. Quisque venenatis sagittis justo a volutpat. Nam libero enim, auctor iaculis velit tincidunt, blandit blandit enim. \n\n\n[^1]: Nulla facilisi. Ut feugiat.\n[^2]: Nullam dui erat, malesuada eget viverra non. [malesuada](https://none)\n[^3]: Phasellus turpis odio, scelerisque eu purus ac\n",[[1524970823304,["ww@DESKTOP-CF9KSFJ",[[-1,56,"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus eu tempor dolor. Suspendisse potenti. Curabitur blandit leo sit amet velit iaculis cursus.\n\n## Donec ornare\n\nPhasellus et massa aliquam, ullamcorper neque eu, dignissim elit. Duis venenatis turpis est. Sed finibus eros eget tellus fringilla, in pulvinar ex lobortis. Praesent vel urna tellus. Aliquam accumsan augue magna, rutrum pulvinar erat luctus sed. Nunc malesuada erat sed nulla pretium maximus. Vivamus consequat convallis porta. Nam congue scelerisque congue.\n\n### Pellentesque viverra\n\nInterdum et malesuada fames ac ante ipsum primis in faucibus. Morbi sagittis aliquam orci, id elementum leo varius at. Suspendisse in hendrerit orci. Aliquam nec congue [^1] augue. In volutpat scelerisque congue. Nam ultricies justo dictum ligula pulvinar iaculis. Etiam vulputate nulla eget lectus fermentum accumsan. Sed in sem tincidunt, sollicitudin erat pharetra, bibendum sapien.\n\n| Maecenas | Molestie | Nisi |\n| ------ | ------ | ------: |\n| Et dolor | Congue ultricies. Phasellus viverra convallis urna | eu |\n| Tempor eros | Sagittis eget. Nunc aliquet dolor a velit euismod vestibulum. Integer libero massa, pharetra sodales lacinia | et |\n| Faucibus | Ornare purus. Etiam vestibulum finibus | urna |\n| Quis | Pulvinar elit luctus sed. Duis vulputate vehicula magna ut congue. Nullam gravida orci ut est placerat, quis vulputate dui ornare. Suspendisse nec sapien id odio dignissim tempor. Aliquam at feugiat purus. Vivamus a euismod urna, in tempor | leo |\n\nSit amet scelerisque nisi interdum. Nulla porttitor [^2], libero sed tempus vulputate, velit nisl suscipit urna, quis dignissim nibh dui sed mi.\n\n### Praesent varius diam\n\nNam id imperdiet turpis. Fusce dignissim vel eros sit amet auctor. Donec quis lorem egestas, placerat odio vel, pulvinar tellus. Nullam tristique neque urna, non dapibus augue pulvinar et. Sed porta ac nulla in ultricies. Aenean ut mollis eros. Aliquam pellentesque vehicula sapien, vel finibus mi efficitur vitae. Donec vulputate aliquet felis sit amet semper. Cras eget odio vel metus bibendum egestas. Pellentesque a turpis sed ipsum tempus condimentum nec bibendum risus.\n\n* * *\n\n## Libero finibus facilisis ac a lacus\n\nInteger tincidunt nunc quis tortor tristique, ut egestas metus volutpat. In sit amet dolor leo. dictum in odio eu vulputate. Nullam a ornare elit, a sollicitudin augue.\n\n * Donec neque velit, gravida eu tempor nec, sagittis ut lectus. Quisque laoreet dignissim facilisis.\n\n * Cras vel rutrum odio, in mollis elit. Maecenas malesuada est sit amet eleifend vulputate. Quisque in ultrices erat.\n\n * Aenean mollis nibh et ligula accumsan semper vitae sed nisi. Ut pulvinar aliquam nulla, id cursus augue tincidunt sed.\n\n### Nullam at erat faucibus \n\nMi interdum sodales. Quisque venenatis sagittis justo a volutpat. Nam libero enim, auctor iaculis velit tincidunt, blandit blandit enim. \n\n"]],[56,2970],[56,56]]],[1524970836873,["ww@DESKTOP-CF9KSFJ",[[1,56,"object detection"]],[56,56],[72,72]]],[1524970837088,["ww@DESKTOP-CF9KSFJ",[[1,73,"\n"]],[72,72],[73,73]]],[1524970853472,["ww@DESKTOP-CF9KSFJ",[[1,73,"Pooling & ReLU"]],[73,73],[87,87]]],[1524970853798,["ww@DESKTOP-CF9KSFJ",[[1,88,"\n"]],[87,87],[88,88]]],[1524970871860,["ww@DESKTOP-CF9KSFJ",[[1,88,"下采样与池化"]],[88,88],[94,94]]],[1524970873366,["ww@DESKTOP-CF9KSFJ",[[1,95,"\n"]],[94,94],[95,95]]],[1524970881125,["ww@DESKTOP-CF9KSFJ",[[1,73,"\n"]],[73,73],[74,74]]],[1524970884519,["ww@DESKTOP-CF9KSFJ",[[1,73,"卷积（）"]],[73,73],[77,77]]],[1524970889691,["ww@DESKTOP-CF9KSFJ",[[1,76,"边缘检测等等"]],[76,76],[82,82]]],[1524970892623,["ww@DESKTOP-CF9KSFJ",[[1,98,"（）"]],[98,98],[100,100]]],[1524970897219,["ww@DESKTOP-CF9KSFJ",[[1,99,"激活函数"]],[99,99],[103,103]]],[1524970919896,["ww@DESKTOP-CF9KSFJ",[[1,113,"\n"]],[111,111],[112,112]]],[1524970924863,["ww@DESKTOP-CF9KSFJ",[[1,112,"VGG16（）"]],[112,112],[119,119]]],[1524970927189,["ww@DESKTOP-CF9KSFJ",[[1,118,"介绍"]],[118,118],[120,120]]],[1524970933960,["ww@DESKTOP-CF9KSFJ",[[1,123,"\n"]],[121,121],[122,122]]],[1524970947089,["ww@DESKTOP-CF9KSFJ",[[1,120,"，卷积神经网络发展"]],[120,120],[129,129]]],[1524970948646,["ww@DESKTOP-CF9KSFJ",[[1,133,"\n"]],[130,130],[131,131]]],[1524970960378,["ww@DESKTOP-CF9KSFJ",[[1,130,"被用作特征提取"]],[130,130],[137,137]]],[1524970974123,["ww@DESKTOP-CF9KSFJ",[[1,131,"很多目标检测"]],[131,131],[137,137]]],[1524970983485,["ww@DESKTOP-CF9KSFJ",[[-1,56,"object detection"]],[72,72],[56,56]]],[1524970996050,["ww@DESKTOP-CF9KSFJ",[[1,56,"## 初步介绍"]],[56,56],[63,63]]],[1524971000976,["ww@DESKTOP-CF9KSFJ",[[-1,22,"Lorem ipsu"]],[22,33],[22,23]]],[1524971001234,["ww@DESKTOP-CF9KSFJ",[[1,23,"u"]],[22,23],[22,24]]],[1524971001497,["ww@DESKTOP-CF9KSFJ",[[1,24,"'b"]],[22,24],[22,26]]],[1524971001650,["ww@DESKTOP-CF9KSFJ",[[1,26,"i"]],[22,26],[22,27]]],[1524971001736,["ww@DESKTOP-CF9KSFJ",[[1,27,"a"]],[22,27],[22,28]]],[1524971002747,["ww@DESKTOP-CF9KSFJ",[[-1,27,"a"]],[22,28],[22,27]]],[1524971002918,["ww@DESKTOP-CF9KSFJ",[[-1,26,"i"]],[22,27],[22,26]]],[1524971003075,["ww@DESKTOP-CF9KSFJ",[[-1,24,"'b"]],[22,26],[22,24]]],[1524971003577,["ww@DESKTOP-CF9KSFJ",[[-1,22,"um"]],[22,24],[22,22]]],[1524971009655,["ww@DESKTOP-CF9KSFJ",[[1,22,"Object detection"]],[22,22],[38,38]]],[1524971036212,["ww@DESKTOP-CF9KSFJ",[[1,141,"## Object detection"]],[141,141],[160,160]]],[1524971037510,["ww@DESKTOP-CF9KSFJ",[[1,162,"\n"]],[160,160],[161,161]]],[1524971045138,["ww@DESKTOP-CF9KSFJ",[[1,140,"16 weght"]],[140,140],[148,148]]],[1524971050305,["ww@DESKTOP-CF9KSFJ",[[1,145,"i"]],[145,145],[146,146]]],[1524971054264,["ww@DESKTOP-CF9KSFJ",[[1,149," layers"]],[149,149],[156,156]]],[1524971060783,["ww@DESKTOP-CF9KSFJ",[[1,140,"* "]],[140,140],[142,142]]],[1524971064414,["ww@DESKTOP-CF9KSFJ",[[1,159,"\n"]],[158,158],[159,159]]],[1524971076077,["ww@DESKTOP-CF9KSFJ",[[1,159,"* 13 convs + 3 fcs"]],[159,159],[177,177]]],[1524971077390,["ww@DESKTOP-CF9KSFJ",[[1,178,"\n"]],[177,177],[178,178]]],[1524971087271,["ww@DESKTOP-CF9KSFJ",[[1,178,"* 5 conv blocks"]],[178,178],[193,193]]],[1524971094141,["ww@DESKTOP-CF9KSFJ",[[1,194,"\n"]],[193,193],[194,194]]],[1524971100966,["ww@DESKTOP-CF9KSFJ",[[1,194,"‘’‘"]],[194,194],[197,197]]],[1524971102294,["ww@DESKTOP-CF9KSFJ",[[-1,194,"‘’‘"]],[197,197],[194,194]]],[1524971103455,["ww@DESKTOP-CF9KSFJ",[[1,194,"···"]],[194,194],[197,197]]],[1524971104925,["ww@DESKTOP-CF9KSFJ",[[-1,194,"···"]],[197,197],[194,194]]],[1524971106965,["ww@DESKTOP-CF9KSFJ",[[1,194,"```"]],[194,194],[197,197]]],[1524971109108,["ww@DESKTOP-CF9KSFJ",[[1,198,"\n"]],[197,197],[198,198]]],[1524971110685,["ww@DESKTOP-CF9KSFJ",[[1,198,"```"]],[198,198],[201,201]]],[1524971113157,["ww@DESKTOP-CF9KSFJ",[[1,198,"\n"]],[197,197],[198,198]]],[1524971123030,["ww@DESKTOP-CF9KSFJ",[[1,197,"python"]],[197,197],[203,203]]],[1524971126133,["ww@DESKTOP-CF9KSFJ",[[1,204,"\t"]],[204,204],[205,205]]],[1524971287511,["ww@DESKTOP-CF9KSFJ",[[1,205,"```text\nINPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n```"]],[205,205],[1842,1842]]],[1524971293764,["ww@DESKTOP-CF9KSFJ",[[-1,1843,"```"]],[1846,1846],[1843,1843]]],[1524971294356,["ww@DESKTOP-CF9KSFJ",[[-1,1843,"\n"]],[1843,1843],[1842,1842]]],[1524971299740,["ww@DESKTOP-CF9KSFJ",[[-1,204,"\t```text"]],[205,212],[204,204]]],[1524971300148,["ww@DESKTOP-CF9KSFJ",[[-1,204,"\n"]],[204,204],[203,203]]],[1524971322853,["ww@DESKTOP-CF9KSFJ",[[1,1833,"``````"]],[1833,1833],[1839,1839]]],[1524971325756,["ww@DESKTOP-CF9KSFJ",[[1,1840,"\n"]],[1839,1839],[1840,1840]]],[1524971328037,["ww@DESKTOP-CF9KSFJ",[[1,1840,"```"]],[1840,1840],[1843,1843]]],[1524971328924,["ww@DESKTOP-CF9KSFJ",[[1,1844,"\n"]],[1843,1843],[1844,1844]]],[1524971329629,["ww@DESKTOP-CF9KSFJ",[[1,1844,"```"]],[1844,1844],[1847,1847]]],[1524971334646,["ww@DESKTOP-CF9KSFJ",[[1,1843,"python"]],[1843,1843],[1849,1849]]],[1524971336868,["ww@DESKTOP-CF9KSFJ",[[1,1850,"\n"]],[1849,1849],[1850,1850]]],[1524971353237,["ww@DESKTOP-CF9KSFJ",[[1,1850,"def "]],[1850,1850],[1854,1854]]],[1524971443925,["ww@DESKTOP-CF9KSFJ",[[1,1881,"\n"]],[1878,1878],[1879,1879]]],[1524971444716,["ww@DESKTOP-CF9KSFJ",[[1,1882,"\n"]],[1879,1879],[1880,1880]]],[1524971451414,["ww@DESKTOP-CF9KSFJ",[[1,1880,"dataset: PASS"]],[1880,1880],[1893,1893]]],[1524971452460,["ww@DESKTOP-CF9KSFJ",[[-1,1892,"S"]],[1893,1893],[1892,1892]]],[1524971457493,["ww@DESKTOP-CF9KSFJ",[[1,1892,"CAL VOC "]],[1892,1892],[1900,1900]]],[1524971458732,["ww@DESKTOP-CF9KSFJ",[[1,1903,"\n"]],[1900,1900],[1901,1901]]],[1524971461262,["ww@DESKTOP-CF9KSFJ",[[1,1901,"tup"]],[1901,1901],[1904,1904]]],[1524971461980,["ww@DESKTOP-CF9KSFJ",[[-1,1901,"tup"]],[1904,1904],[1901,1901]]],[1524971468969,["ww@DESKTOP-CF9KSFJ",[[1,1901,"图片信息更加复杂"]],[1901,1901],[1909,1909]]],[1524971475441,["ww@DESKTOP-CF9KSFJ",[[1,1905,"相对imageNet"]],[1905,1905],[1915,1915]]],[1524971477772,["ww@DESKTOP-CF9KSFJ",[[1,1922,"\n"]],[1919,1919],[1920,1920]]],[1524971498966,["ww@DESKTOP-CF9KSFJ",[[1,1920,"思路："]],[1920,1920],[1923,1923]]],[1524971502460,["ww@DESKTOP-CF9KSFJ",[[1,1920,"\n"]],[1920,1920],[1921,1921]]],[1524971505701,["ww@DESKTOP-CF9KSFJ",[[1,1921,"### "]],[1921,1921],[1925,1925]]],[1524971507531,["ww@DESKTOP-CF9KSFJ",[[1,1931,"\n"]],[1928,1928],[1929,1929]]],[1524971515146,["ww@DESKTOP-CF9KSFJ",[[1,1929,"滑动窗口"]],[1929,1929],[1933,1933]]],[1524971547388,["ww@DESKTOP-CF9KSFJ",[[1,1936,"\n"]],[1933,1933],[1934,1934]]],[1524971644241,["ww@DESKTOP-CF9KSFJ",[[1,1935,"![Lab-object-01]($res/Lab-object-01.png)\n\n"]],[1934,1934],[1976,1976]]],[1524971661963,["ww@DESKTOP-CF9KSFJ",[[1,1979,"\n"]],[1975,1975],[1976,1976]]],[1524971671725,["ww@DESKTOP-CF9KSFJ",[[1,1976,"穷举来实现"]],[1976,1976],[1981,1981]]],[1524971681746,["ww@DESKTOP-CF9KSFJ",[[1,1929,"\n"]],[1929,1929],[1930,1930]]],[1524971688652,["ww@DESKTOP-CF9KSFJ",[[1,1929,"1.暴力解决："]],[1929,1929],[1936,1936]]],[1524971694386,["ww@DESKTOP-CF9KSFJ",[[-1,1942,"\n"]],[1942,1942],[1941,1941]]],[1524971699364,["ww@DESKTOP-CF9KSFJ",[[1,1941,"（）"]],[1941,1941],[1943,1943]]],[1524971702969,["ww@DESKTOP-CF9KSFJ",[[1,1942,"已经猜到"]],[1942,1942],[1946,1946]]],[1524971706082,["ww@DESKTOP-CF9KSFJ",[[-1,1942,"已经猜到"]],[1946,1946],[1942,1942]]],[1524971713202,["ww@DESKTOP-CF9KSFJ",[[1,1942,"已经猜到人脸大小"]],[1942,1942],[1950,1950]]],[1524971718719,["ww@DESKTOP-CF9KSFJ",[[1,1950,"设定窗口大小"]],[1950,1950],[1956,1956]]],[1524971729992,["ww@DESKTOP-CF9KSFJ",[[1,2004,"，每个像素便利"]],[2004,2004],[2011,2011]]],[1524971730971,["ww@DESKTOP-CF9KSFJ",[[-1,2009,"便利"]],[2011,2011],[2009,2009]]],[1524971733766,["ww@DESKTOP-CF9KSFJ",[[1,2009,"遍历"]],[2009,2009],[2011,2011]]],[1524971734682,["ww@DESKTOP-CF9KSFJ",[[1,2015,"\n"]],[2011,2011],[2012,2012]]],[1524971781588,["ww@DESKTOP-CF9KSFJ",[[1,1942,"（）"]],[1941,1941],[1943,1943]]],[1524971783067,["ww@DESKTOP-CF9KSFJ",[[-1,1941,"（）"]],[1943,1943],[1941,1941]]],[1524971796588,["ww@DESKTOP-CF9KSFJ",[[1,2012,"2.Region Proposal（）"]],[2012,2012],[2031,2031]]],[1524971802016,["ww@DESKTOP-CF9KSFJ",[[1,2030,"区域提名"]],[2030,2030],[2034,2034]]],[1524971803051,["ww@DESKTOP-CF9KSFJ",[[1,2039,"\n"]],[2035,2035],[2036,2036]]],[1524971806225,["ww@DESKTOP-CF9KSFJ",[[1,2036,"选择性搜索"]],[2036,2036],[2041,2041]]],[1524971806986,["ww@DESKTOP-CF9KSFJ",[[1,2045,"\n"]],[2041,2041],[2042,2042]]],[1524972436758,[null,[[-1,1941,"）"],[1,1942,"（"]],[1941,1941],[1943,1943]]],[1524972436758,[null,[[1,1941,"）"],[-1,1941,"（"]],[1943,1943],[1941,1941]]],[1524972399761,["ww@DESKTOP-CF9KSFJ",[[-1,1936,"\n"]],[1937,1937],[1936,1936]]],[1524972409400,["ww@DESKTOP-CF9KSFJ",[[1,1940,"\n"]],[1940,1940],[1941,1941]]],[1524972496748,[null,[[-1,1941,"）"],[1,1942,"（"]],[1941,1941],[1943,1943]]],[1524972496748,[null,[[1,1941,"）"],[-1,1941,"（"]],[1943,1943],[1941,1941]]],[1524972468043,["ww@DESKTOP-CF9KSFJ",[[1,2042,"color"]],[2042,2042],[2047,2047]]],[1524972469024,["ww@DESKTOP-CF9KSFJ",[[1,2051,"\n"]],[2047,2047],[2048,2048]]],[1524972472315,["ww@DESKTOP-CF9KSFJ",[[1,2048,"texture"]],[2048,2048],[2055,2055]]],[1524972472848,["ww@DESKTOP-CF9KSFJ",[[1,2059,"\n"]],[2055,2055],[2056,2056]]],[1524972475461,["ww@DESKTOP-CF9KSFJ",[[1,2056,"size"]],[2056,2056],[2060,2060]]],[1524972476144,["ww@DESKTOP-CF9KSFJ",[[1,2064,"\n"]],[2060,2060],[2061,2061]]],[1524972477986,["ww@DESKTOP-CF9KSFJ",[[1,2061,"fill"]],[2061,2061],[2065,2065]]],[1524972556742,[null,[[-1,1941,"）"],[1,1942,"（"]],[1941,1941],[1943,1943]]],[1524972556742,[null,[[1,1941,"）"],[-1,1941,"（"]],[1943,1943],[1941,1941]]],[1524972553761,["ww@DESKTOP-CF9KSFJ",[[1,2069,"\n"]],[2065,2065],[2066,2066]]],[1524972616744,[null,[[-1,1941,"）"],[1,1942,"（"]],[1941,1941],[1943,1943]]],[1524972616744,[null,[[1,1941,"）"],[-1,1941,"（"]],[1943,1943],[1941,1941]]],[1524972565497,["ww@DESKTOP-CF9KSFJ",[[1,2061,"![Lab-object-02]($res/Lab-object-02.png)\n\n"]],[2066,2066],[2102,2102]]],[1524972569160,["ww@DESKTOP-CF9KSFJ",[[-1,2103,"fill"]],[2103,2107],[2103,2103]]],[1524972570784,["ww@DESKTOP-CF9KSFJ",[[1,2061,"\n"]],[2060,2060],[2061,2061]]],[1524972571151,["ww@DESKTOP-CF9KSFJ",[[1,2061,"fill"]],[2061,2061],[2065,2065]]],[1524972587961,["ww@DESKTOP-CF9KSFJ",[[1,2041,"（0"]],[2041,2041],[2043,2043]]],[1524972589303,["ww@DESKTOP-CF9KSFJ",[[-1,2041,"（0"]],[2043,2043],[2041,2041]]],[1524972590153,["ww@DESKTOP-CF9KSFJ",[[1,2041,"（）"]],[2041,2041],[2043,2043]]],[1524972604758,["ww@DESKTOP-CF9KSFJ",[[1,2042,"颜色、纹理、尺度、包含关系"]],[2042,2042],[2055,2055]]],[1524972607008,["ww@DESKTOP-CF9KSFJ",[[1,2081,"\n"]],[2080,2080],[2081,2081]]],[1524972610677,["ww@DESKTOP-CF9KSFJ",[[1,2081,"进行合并"]],[2081,2081],[2085,2085]]],[1524972676746,[null,[[-1,1941,"）"],[1,1942,"（"]],[1941,1941],[1943,1943]]],[1524972676746,[null,[[1,1941,"）"],[-1,1941,"（"]],[1943,1943],[1941,1941]]],[1524972634670,["ww@DESKTOP-CF9KSFJ",[[1,2127,"由色泽不同i你"]],[2127,2127],[2134,2134]]],[1524972635327,["ww@DESKTOP-CF9KSFJ",[[-1,2132,"i你"]],[2134,2134],[2132,2132]]],[1524972654858,["ww@DESKTOP-CF9KSFJ",[[1,2132,"和图像之间的相似度分成几个大块儿，"]],[2132,2132],[2149,2149]]],[1524972736750,[null,[[-1,1941,"）"],[1,1942,"（"]],[1941,1941],[1943,1943]]],[1524972736750,[null,[[1,1941,"）"],[-1,1941,"（"]],[1943,1943],[1941,1941]]],[1524972709532,["ww@DESKTOP-CF9KSFJ",[[1,2127,"光晕部分"]],[2127,2127],[2131,2131]]],[1524972725484,["ww@DESKTOP-CF9KSFJ",[[1,2145,"，对其进行合并，最终"]],[2145,2145],[2155,2155]]],[1524972730852,["ww@DESKTOP-CF9KSFJ",[[1,2163,"留下"]],[2163,2163],[2165,2165]]],[1524972796748,[null,[[-1,1941,"）"],[1,1942,"（"]],[1941,1941],[1943,1943]]],[1524972796748,[null,[[1,1941,"）"],[-1,1941,"（"]],[1943,1943],[1941,1941]]],[1524972739510,["ww@DESKTOP-CF9KSFJ",[[-1,2163,"留下"]],[2165,2165],[2163,2163]]],[1524972856749,[null,[[-1,1941,"）"],[1,1942,"（"]],[1941,1941],[1943,1943]]],[1524972856749,[null,[[1,1941,"）"],[-1,1941,"（"]],[1943,1943],[1941,1941]]],[1524972811532,["ww@DESKTOP-CF9KSFJ",[[1,2163,"对应到下面就是这个蓝色的块儿，及时"]],[2163,2163],[2180,2180]]],[1524972813272,["ww@DESKTOP-CF9KSFJ",[[-1,2178,"及时"]],[2180,2180],[2178,2178]]],[1524972831720,["ww@DESKTOP-CF9KSFJ",[[1,2178,"就是猜测到的有可能有目标物体的图块儿。"]],[2178,2178],[2197,2197]]],[1524972916752,[null,[[-1,1941,"）"],[1,1942,"（"]],[1941,1941],[1943,1943]]],[1524972916752,[null,[[1,1941,"）"],[-1,1941,"（"]],[1943,1943],[1941,1941]]],[1524972857494,["ww@DESKTOP-CF9KSFJ",[[1,2197,"然后对几个"]],[2197,2197],[2202,2202]]],[1524972862196,["ww@DESKTOP-CF9KSFJ",[[1,2200,"候选的"]],[2200,2200],[2203,2203]]],[1524972874581,["ww@DESKTOP-CF9KSFJ",[[1,2205,"框进行分类，这样极大提高了晕眩"]],[2205,2205],[2220,2220]]],[1524972875150,["ww@DESKTOP-CF9KSFJ",[[-1,2218,"晕眩"]],[2220,2220],[2218,2218]]],[1524972882716,["ww@DESKTOP-CF9KSFJ",[[1,2218,"速度，从琼剧"]],[2218,2218],[2224,2224]]],[1524972883478,["ww@DESKTOP-CF9KSFJ",[[-1,2222,"琼剧"]],[2224,2224],[2222,2222]]],[1524972897273,["ww@DESKTOP-CF9KSFJ",[[1,2222,"穷举变成有限次的分类问题。"]],[2222,2222],[2235,2235]]],[1524972976754,[null,[[-1,1941,"）"],[1,1942,"（"]],[1941,1941],[1943,1943]]],[1524972976754,[null,[[1,1941,"）"],[-1,1941,"（"]],[1943,1943],[1941,1941]]],[1524972919679,["ww@DESKTOP-CF9KSFJ",[[1,1929,"#### "]],[1929,1929],[1934,1934]]],[1524972925192,["ww@DESKTOP-CF9KSFJ",[[1,2017,"#####"]],[2017,2017],[2022,2022]]],[1524972926567,["ww@DESKTOP-CF9KSFJ",[[-1,2021,"#"]],[2022,2022],[2021,2021]]],[1524972927039,["ww@DESKTOP-CF9KSFJ",[[1,2021," "]],[2021,2021],[2022,2022]]],[1524972932942,["ww@DESKTOP-CF9KSFJ",[[1,2251,"\n"]],[2245,2245],[2246,2246]]],[1524973036754,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524973036754,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524972989021,["ww@DESKTOP-CF9KSFJ",[[1,2246,"类似层次聚类，本身是无监督的"]],[2246,2246],[2260,2260]]],[1524972990022,["ww@DESKTOP-CF9KSFJ",[[1,2266,"\n"]],[2260,2260],[2261,2261]]],[1524973016806,["ww@DESKTOP-CF9KSFJ",[[1,2261,"一张图片 以提名的方式提出大概有目标物体的几个块儿，然后用分类的方法去"]],[2261,2261],[2296,2296]]],[1524973025943,["ww@DESKTOP-CF9KSFJ",[[1,2302,"\n"]],[2296,2296],[2297,2297]]],[1524973096757,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524973096757,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524973038119,["ww@DESKTOP-CF9KSFJ",[[1,2303,"\n"]],[2296,2296],[2297,2297]]],[1524973051244,["ww@DESKTOP-CF9KSFJ",[[1,2297,"SSD Driven clustering"]],[2297,2297],[2318,2318]]],[1524973059872,["ww@DESKTOP-CF9KSFJ",[[1,2308,"Hierarchical "]],[2308,2308],[2321,2321]]],[1524973071285,["ww@DESKTOP-CF9KSFJ",[[1,2296,"看看属于哪一类"]],[2296,2296],[2303,2303]]],[1524973075389,["ww@DESKTOP-CF9KSFJ",[[-1,2282,"几个块儿"]],[2286,2286],[2282,2282]]],[1524973076772,["ww@DESKTOP-CF9KSFJ",[[1,2282,"提名"]],[2282,2282],[2284,2284]]],[1524973078518,["ww@DESKTOP-CF9KSFJ",[[1,2343,"\n"]],[2336,2336],[2337,2337]]],[1524973078733,["ww@DESKTOP-CF9KSFJ",[[1,2344,"\n"]],[2337,2337],[2338,2338]]],[1524973093379,["ww@DESKTOP-CF9KSFJ",[[1,2338,"用卷积神经网络CNN，在这里就是用"]],[2338,2338],[2355,2355]]],[1524973156756,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524973156756,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524973104703,["ww@DESKTOP-CF9KSFJ",[[1,2355,"Region-based CNNN’="]],[2355,2355],[2374,2374]]],[1524973105693,["ww@DESKTOP-CF9KSFJ",[[-1,2371,"N’="]],[2374,2374],[2371,2371]]],[1524973107382,["ww@DESKTOP-CF9KSFJ",[[1,2371," （）"]],[2371,2371],[2374,2374]]],[1524973109953,["ww@DESKTOP-CF9KSFJ",[[1,2373,"RCNN'"]],[2373,2373],[2378,2378]]],[1524973110821,["ww@DESKTOP-CF9KSFJ",[[-1,2377,"'"]],[2378,2378],[2377,2377]]],[1524973111701,["ww@DESKTOP-CF9KSFJ",[[1,2385,"\n"]],[2378,2378],[2379,2379]]],[1524973112213,["ww@DESKTOP-CF9KSFJ",[[1,2386,"\n"]],[2379,2379],[2380,2380]]],[1524973216760,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524973216760,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524973180366,["ww@DESKTOP-CF9KSFJ",[[1,2387,"\n"]],[2378,2378],[2379,2379]]],[1524973191882,["ww@DESKTOP-CF9KSFJ",[[1,2261,"![Lab-object-03]($res/Lab-object-03.png)\n\n"]],[2380,2380],[2302,2302]]],[1524973195693,["ww@DESKTOP-CF9KSFJ",[[-1,2261,"![Lab-object-03]($res/Lab-object-03.png)"]],[2261,2301],[2261,2261]]],[1524973198157,["ww@DESKTOP-CF9KSFJ",[[1,2381,"![Lab-object-03]($res/Lab-object-03.png)"]],[2381,2381],[2421,2421]]],[1524973636774,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524973636774,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524973635882,["ww@DESKTOP-CF9KSFJ",[[1,2422,"流程"]],[2422,2422],[2424,2424]]],[1524973696762,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524973696762,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524973636844,["ww@DESKTOP-CF9KSFJ",[[1,2424,";"]],[2424,2424],[2425,2425]]],[1524973637012,["ww@DESKTOP-CF9KSFJ",[[1,2433,"\n"]],[2425,2425],[2426,2426]]],[1524973637835,["ww@DESKTOP-CF9KSFJ",[[-1,2433,"\n"]],[2426,2426],[2425,2425]]],[1524973638252,["ww@DESKTOP-CF9KSFJ",[[-1,2424,";"]],[2425,2425],[2424,2424]]],[1524973638957,["ww@DESKTOP-CF9KSFJ",[[1,2424,":"]],[2424,2424],[2425,2425]]],[1524973639155,["ww@DESKTOP-CF9KSFJ",[[1,2433,"\n"]],[2425,2425],[2426,2426]]],[1524973641477,["ww@DESKTOP-CF9KSFJ",[[1,2426,"1.shu"]],[2426,2426],[2431,2431]]],[1524973642324,["ww@DESKTOP-CF9KSFJ",[[-1,2428,"shu"]],[2431,2431],[2428,2428]]],[1524973646353,["ww@DESKTOP-CF9KSFJ",[[1,2428,"输入图像"]],[2428,2428],[2432,2432]]],[1524973647308,["ww@DESKTOP-CF9KSFJ",[[1,2440,"\n"]],[2432,2432],[2433,2433]]],[1524973667554,["ww@DESKTOP-CF9KSFJ",[[1,2433,"2.每张图像生成1K~2K个候选区域"]],[2433,2433],[2451,2451]]],[1524973667868,["ww@DESKTOP-CF9KSFJ",[[1,2459,"\n"]],[2451,2451],[2452,2452]]],[1524973672338,["ww@DESKTOP-CF9KSFJ",[[1,2452,"3.对每张"]],[2452,2452],[2457,2457]]],[1524973673811,["ww@DESKTOP-CF9KSFJ",[[-1,2455,"每张"]],[2457,2457],[2455,2455]]],[1524973692809,["ww@DESKTOP-CF9KSFJ",[[1,2455,"每个候选区域，使用深度网络提取特征"]],[2455,2455],[2472,2472]]],[1524973756765,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524973756765,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524973699173,["ww@DESKTOP-CF9KSFJ",[[1,2472,"（）"]],[2472,2472],[2474,2474]]],[1524973716317,["ww@DESKTOP-CF9KSFJ",[[1,2473,"AlexNet、VGG、CNN......."]],[2473,2473],[2495,2495]]],[1524973719210,["ww@DESKTOP-CF9KSFJ",[[-1,2494,"."]],[2495,2495],[2494,2494]]],[1524973720051,["ww@DESKTOP-CF9KSFJ",[[1,2503,"\n"]],[2495,2495],[2496,2496]]],[1524973749979,["ww@DESKTOP-CF9KSFJ",[[1,2496,"4.将特征送入每一类的SVM分类器，判别是否属于该类"]],[2496,2496],[2522,2522]]],[1524973750467,["ww@DESKTOP-CF9KSFJ",[[1,2530,"\n"]],[2522,2522],[2523,2523]]],[1524973753962,["ww@DESKTOP-CF9KSFJ",[[1,2523,"5.使用"]],[2523,2523],[2527,2527]]],[1524973816767,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524973816767,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524973772770,["ww@DESKTOP-CF9KSFJ",[[1,2527,"回归器精细修正候选框位置"]],[2527,2527],[2539,2539]]],[1524973774627,["ww@DESKTOP-CF9KSFJ",[[1,2547,"\n"]],[2539,2539],[2540,2540]]],[1524973807229,["ww@DESKTOP-CF9KSFJ",[[1,2548,"\n"]],[2539,2539],[2540,2540]]],[1524973876766,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524973876766,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524973821220,["ww@DESKTOP-CF9KSFJ",[[1,2549,"\n"]],[2540,2540],[2541,2541]]],[1524973936771,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524973936771,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524973935568,["ww@DESKTOP-CF9KSFJ",[[1,2541,"pretrained with RCNN"]],[2541,2541],[2561,2561]]],[1524973996770,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524973996770,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524973936908,["ww@DESKTOP-CF9KSFJ",[[1,2561,"（）"]],[2561,2561],[2563,2563]]],[1524973940007,["ww@DESKTOP-CF9KSFJ",[[1,2562,"用"]],[2562,2562],[2563,2563]]],[1524974056773,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524974056773,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524974012364,["ww@DESKTOP-CF9KSFJ",[[1,2498,"1"]],[2498,2498],[2499,2499]]],[1524974014411,["ww@DESKTOP-CF9KSFJ",[[-1,2524,"5"]],[2525,2525],[2524,2524]]],[1524974015204,["ww@DESKTOP-CF9KSFJ",[[1,2524,"4"]],[2524,2524],[2525,2525]]],[1524974016509,["ww@DESKTOP-CF9KSFJ",[[1,2526,"2"]],[2526,2526],[2527,2527]]],[1524974018613,["ww@DESKTOP-CF9KSFJ",[[1,2542,"5."]],[2542,2542],[2544,2544]]],[1524974029359,["ww@DESKTOP-CF9KSFJ",[[1,2567,"imagenet"]],[2567,2567],[2575,2575]]],[1524974031115,["ww@DESKTOP-CF9KSFJ",[[-1,2572,"n"]],[2573,2573],[2572,2572]]],[1524974031861,["ww@DESKTOP-CF9KSFJ",[[1,2572,"N"]],[2572,2572],[2573,2573]]],[1524974040455,["ww@DESKTOP-CF9KSFJ",[[1,2567,"VGG16在"]],[2567,2567],[2573,2573]]],[1524974043179,["ww@DESKTOP-CF9KSFJ",[[1,2591,"\n"]],[2582,2582],[2583,2583]]],[1524974056613,["ww@DESKTOP-CF9KSFJ",[[1,2583,"嫁接到Pascal上，不用输出1000"]],[2583,2583],[2602,2602]]],[1524974116773,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524974116773,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524974065116,["ww@DESKTOP-CF9KSFJ",[[1,2602,"类，输出21类即可（）"]],[2602,2602],[2613,2613]]],[1524974077232,["ww@DESKTOP-CF9KSFJ",[[1,2612,"加一个不再考虑的类别"]],[2612,2612],[2622,2622]]],[1524974079346,["ww@DESKTOP-CF9KSFJ",[[-1,2616,"再"]],[2617,2617],[2616,2616]]],[1524974080063,["ww@DESKTOP-CF9KSFJ",[[1,2616,"在"]],[2616,2616],[2617,2617]]],[1524974082473,["ww@DESKTOP-CF9KSFJ",[[1,2632,"\n"]],[2623,2623],[2624,2624]]],[1524974091968,["ww@DESKTOP-CF9KSFJ",[[1,2624,"改下"]],[2624,2624],[2626,2626]]],[1524974093873,["ww@DESKTOP-CF9KSFJ",[[-1,2624,"改下"]],[2626,2626],[2624,2624]]],[1524974097798,["ww@DESKTOP-CF9KSFJ",[[1,2624,"在对"]],[2624,2624],[2626,2626]]],[1524974098273,["ww@DESKTOP-CF9KSFJ",[[-1,2624,"在对"]],[2626,2626],[2624,2624]]],[1524974104904,["ww@DESKTOP-CF9KSFJ",[[1,2624,"对最后的全连接成"]],[2624,2624],[2632,2632]]],[1524974105201,["ww@DESKTOP-CF9KSFJ",[[-1,2631,"成"]],[2632,2632],[2631,2631]]],[1524974106630,["ww@DESKTOP-CF9KSFJ",[[1,2631,"曾"]],[2631,2631],[2632,2632]]],[1524974107281,["ww@DESKTOP-CF9KSFJ",[[-1,2631,"曾"]],[2632,2632],[2631,2631]]],[1524974115896,["ww@DESKTOP-CF9KSFJ",[[1,2631,"层进行稍微改造"]],[2631,2631],[2638,2638]]],[1524974176774,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524974176774,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524974119993,["ww@DESKTOP-CF9KSFJ",[[1,2647,"\n"]],[2638,2638],[2639,2639]]],[1524974120994,["ww@DESKTOP-CF9KSFJ",[[1,2648,"\n"]],[2639,2639],[2640,2640]]],[1524974236777,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524974236777,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524974202617,["ww@DESKTOP-CF9KSFJ",[[1,2624,"拿出卷积部分，再"]],[2624,2624],[2632,2632]]],[1524974206400,["ww@DESKTOP-CF9KSFJ",[[1,2646,"即可"]],[2646,2646],[2648,2648]]],[1524974206850,["ww@DESKTOP-CF9KSFJ",[[1,2659,"\n"]],[2648,2648],[2649,2649]]],[1524974207322,["ww@DESKTOP-CF9KSFJ",[[1,2660,"\n"]],[2649,2649],[2650,2650]]],[1524974233867,["ww@DESKTOP-CF9KSFJ",[[1,2650,"对一张图要提名的太多2000，"]],[2650,2650],[2665,2665]]],[1524974296777,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524974296777,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524974252191,["ww@DESKTOP-CF9KSFJ",[[1,2665,"然后对每一个提名做卷积，也就是2000此"]],[2665,2665],[2685,2685]]],[1524974252842,["ww@DESKTOP-CF9KSFJ",[[-1,2684,"此"]],[2685,2685],[2684,2684]]],[1524974262274,["ww@DESKTOP-CF9KSFJ",[[1,2684,"次，那么运行效率会比较慢"]],[2684,2684],[2696,2696]]],[1524974263010,["ww@DESKTOP-CF9KSFJ",[[1,2707,"\n"]],[2696,2696],[2697,2697]]],[1524974263402,["ww@DESKTOP-CF9KSFJ",[[1,2708,"\n"]],[2697,2697],[2698,2698]]],[1524974266434,["ww@DESKTOP-CF9KSFJ",[[-1,2708,"\n"]],[2698,2698],[2697,2697]]],[1524974274334,["ww@DESKTOP-CF9KSFJ",[[1,2697,"引出Fast-RCNN"]],[2697,2697],[2708,2708]]],[1524974277100,["ww@DESKTOP-CF9KSFJ",[[1,2705,"-"]],[2705,2705],[2706,2706]]],[1524974279634,["ww@DESKTOP-CF9KSFJ",[[-1,2703,"-"]],[2704,2704],[2703,2703]]],[1524974279876,["ww@DESKTOP-CF9KSFJ",[[1,2703," "]],[2703,2703],[2704,2704]]],[1524974281681,["ww@DESKTOP-CF9KSFJ",[[1,2720,"\n"]],[2709,2709],[2710,2710]]],[1524974282178,["ww@DESKTOP-CF9KSFJ",[[1,2721,"\n"]],[2710,2710],[2711,2711]]],[1524974287563,["ww@DESKTOP-CF9KSFJ",[[1,2711,"#### "]],[2711,2711],[2716,2716]]],[1524974356779,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524974356779,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524974300821,["ww@DESKTOP-CF9KSFJ",[[1,2716,"Fast R-CNN"]],[2716,2716],[2726,2726]]],[1524974306451,["ww@DESKTOP-CF9KSFJ",[[1,2715,"#"]],[2715,2715],[2716,2716]]],[1524974312210,["ww@DESKTOP-CF9KSFJ",[[1,2738,"\n"]],[2727,2727],[2728,2728]]],[1524974324688,["ww@DESKTOP-CF9KSFJ",[[1,2676,"提取特征"]],[2676,2676],[2680,2680]]],[1524974330745,["ww@DESKTOP-CF9KSFJ",[[1,2743,"\n"]],[2732,2732],[2733,2733]]],[1524974335177,["ww@DESKTOP-CF9KSFJ",[[-1,2743,"\n"]],[2733,2733],[2732,2732]]],[1524974347328,["ww@DESKTOP-CF9KSFJ",[[1,2732,"堆出来"]],[2732,2732],[2735,2735]]],[1524974348625,["ww@DESKTOP-CF9KSFJ",[[-1,2732,"堆出来"]],[2735,2735],[2732,2732]]],[1524974356527,["ww@DESKTOP-CF9KSFJ",[[1,2732,"对特征映射做"]],[2732,2732],[2738,2738]]],[1524974416779,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524974416779,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524974361563,["ww@DESKTOP-CF9KSFJ",[[1,2738,"区域提名，"]],[2738,2738],[2743,2743]]],[1524974409204,["ww@DESKTOP-CF9KSFJ",[[1,2732,"一个图片的"]],[2732,2732],[2737,2737]]],[1524974410304,["ww@DESKTOP-CF9KSFJ",[[-1,2736,"的"]],[2737,2737],[2736,2736]]],[1524974415586,["ww@DESKTOP-CF9KSFJ",[[1,2736,"经过特征映射，"]],[2736,2736],[2743,2743]]],[1524974476781,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524974476781,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524974439839,["ww@DESKTOP-CF9KSFJ",[[1,2743,"信息量降低了，但对物体的分类的重要信息都保留了，所以只"]],[2743,2743],[2770,2770]]],[1524974474870,["ww@DESKTOP-CF9KSFJ",[[1,2781,"经一部"]],[2781,2781],[2784,2784]]],[1524974476408,["ww@DESKTOP-CF9KSFJ",[[-1,2781,"经一部"]],[2784,2784],[2781,2781]]],[1524974536785,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524974536785,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524974492623,["ww@DESKTOP-CF9KSFJ",[[1,2781,"进一步的卷积提取特征，减少了计算量"]],[2781,2781],[2798,2798]]],[1524974549635,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524974549635,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524974547883,["ww@DESKTOP-CF9KSFJ",[[1,2798,"。"]],[2798,2798],[2799,2799]]],[1524974609638,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524974609638,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524974609297,["ww@DESKTOP-CF9KSFJ",[[1,2801,"#####"]],[2801,2801],[2806,2806]]],[1524974669640,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1524974669640,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1524974612019,["ww@DESKTOP-CF9KSFJ",[[1,2806,"YOLO"]],[2806,2806],[2810,2810]]],[1524974614992,["ww@DESKTOP-CF9KSFJ",[[1,2806," "]],[2806,2806],[2807,2807]]],[1524974616623,["ww@DESKTOP-CF9KSFJ",[[1,2820,"\n"]],[2811,2811],[2812,2812]]],[1524974616783,["ww@DESKTOP-CF9KSFJ",[[1,2821,"\n"]],[2812,2812],[2813,2813]]],[1525156609654,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525156609654,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525156596163,["ww@DESKTOP-CF9KSFJ",[[1,2738,"卷积后的"]],[2738,2738],[2742,2742]]],[1525156601775,["ww@DESKTOP-CF9KSFJ",[[1,2769,"下来"]],[2769,2769],[2771,2771]]],[1525156669631,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525156669631,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525156639173,["ww@DESKTOP-CF9KSFJ",[[1,2798,"不在原图上做区域提名，因此"]],[2798,2798],[2811,2811]]],[1525156789647,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525156789647,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525156735516,["ww@DESKTOP-CF9KSFJ",[[1,2818,"同时seleced"]],[2818,2818],[2827,2827]]],[1525156736461,["ww@DESKTOP-CF9KSFJ",[[-1,2825,"ed"]],[2827,2827],[2825,2825]]],[1525156742914,["ww@DESKTOP-CF9KSFJ",[[1,2825,"ted search智能"]],[2825,2825],[2837,2837]]],[1525156743765,["ww@DESKTOP-CF9KSFJ",[[-1,2835,"智能"]],[2837,2837],[2835,2835]]],[1525156748396,["ww@DESKTOP-CF9KSFJ",[[1,2835,"只能在脆皮"]],[2835,2835],[2840,2840]]],[1525156750550,["ww@DESKTOP-CF9KSFJ",[[-1,2838,"脆皮"]],[2840,2840],[2838,2838]]],[1525156756026,["ww@DESKTOP-CF9KSFJ",[[1,2838,"cpu上运行"]],[2838,2838],[2844,2844]]],[1525156775813,["ww@DESKTOP-CF9KSFJ",[[-1,2818,"同时selected search只能在cpu上运行"]],[2844,2844],[2818,2818]]],[1525156776829,["ww@DESKTOP-CF9KSFJ",[[1,2820,"\n"]],[2818,2818],[2819,2819]]],[1525156777869,["ww@DESKTOP-CF9KSFJ",[[1,2821,"\n"]],[2819,2819],[2820,2820]]],[1525156784824,["ww@DESKTOP-CF9KSFJ",[[1,2820,"\tselected search"]],[2820,2820],[2836,2836]]],[1525156785533,["ww@DESKTOP-CF9KSFJ",[[1,2838,"\n"]],[2836,2836],[2837,2837]]],[1525156788945,["ww@DESKTOP-CF9KSFJ",[[1,2837,"Faster"]],[2837,2837],[2843,2843]]],[1525156849633,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525156849633,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525156792329,["ww@DESKTOP-CF9KSFJ",[[1,2843," RCNN"]],[2843,2843],[2848,2848]]],[1525156792893,["ww@DESKTOP-CF9KSFJ",[[1,2850,"\n"]],[2848,2848],[2849,2849]]],[1525156843898,["ww@DESKTOP-CF9KSFJ",[[1,2836,"是在cpu上运行的，因此想到对区域提名部分再做一个卷积操作，使整个操作都你们个"]],[2836,2836],[2875,2875]]],[1525156844949,["ww@DESKTOP-CF9KSFJ",[[-1,2871,"都你们个"]],[2875,2875],[2871,2871]]],[1525156849091,["ww@DESKTOP-CF9KSFJ",[[1,2871,"都能在GPU"]],[2871,2871],[2877,2877]]],[1525156855577,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525156855577,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525156851707,["ww@DESKTOP-CF9KSFJ",[[1,2877,"上运行"]],[2877,2877],[2880,2880]]],[1525156852429,["ww@DESKTOP-CF9KSFJ",[[-1,2878,"运行"]],[2880,2880],[2878,2878]]],[1525156855104,["ww@DESKTOP-CF9KSFJ",[[1,2878,"完成。"]],[2878,2878],[2881,2881]]],[1525156915582,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525156915582,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525156867118,["ww@DESKTOP-CF9KSFJ",[[1,2882,"##### "]],[2882,2882],[2888,2888]]],[1525156975583,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525156975583,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525156938557,["ww@DESKTOP-CF9KSFJ",[[1,2902,"\n"]],[2899,2899],[2900,2900]]],[1525156938709,["ww@DESKTOP-CF9KSFJ",[[1,2903,"\n"]],[2900,2900],[2901,2901]]],[1525156949773,["ww@DESKTOP-CF9KSFJ",[[1,2901,"一眼看kuan"]],[2901,2901],[2908,2908]]],[1525156950708,["ww@DESKTOP-CF9KSFJ",[[-1,2904,"kuan"]],[2908,2908],[2904,2904]]],[1525156957731,["ww@DESKTOP-CF9KSFJ",[[1,2904,"框打在哪儿"]],[2904,2904],[2909,2909]]],[1525156961703,["ww@DESKTOP-CF9KSFJ",[[1,2903,"："]],[2903,2903],[2904,2904]]],[1525156964640,["ww@DESKTOP-CF9KSFJ",[[1,2910,"。"]],[2910,2910],[2911,2911]]],[1525156967309,["ww@DESKTOP-CF9KSFJ",[[1,2914,"\n"]],[2911,2911],[2912,2912]]],[1525156970655,["ww@DESKTOP-CF9KSFJ",[[1,2912,"两眼："]],[2912,2912],[2915,2915]]],[1525157035582,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525157035582,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525156993375,["ww@DESKTOP-CF9KSFJ",[[1,2916,"为了考虑实时性，"]],[2916,2916],[2924,2924]]],[1525156995509,["ww@DESKTOP-CF9KSFJ",[[-1,2923,"，"]],[2924,2924],[2923,2923]]],[1525157005486,["ww@DESKTOP-CF9KSFJ",[[1,2923,"，达到在线预测的目的。"]],[2923,2923],[2934,2934]]],[1525157011853,["ww@DESKTOP-CF9KSFJ",[[1,2916,"\n"]],[2916,2916],[2917,2917]]],[1525157035469,["ww@DESKTOP-CF9KSFJ",[[1,2900,"两个分支;quyutiming"]],[2900,2900],[2915,2915]]],[1525157095585,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525157095585,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525157037725,["ww@DESKTOP-CF9KSFJ",[[-1,2905,"quyutiming"]],[2915,2915],[2905,2905]]],[1525157044618,["ww@DESKTOP-CF9KSFJ",[[1,2905,"区域提名，狂位置"]],[2905,2905],[2913,2913]]],[1525157055780,["ww@DESKTOP-CF9KSFJ",[[-1,2909,"，狂位置"]],[2913,2913],[2909,2909]]],[1525157063722,["ww@DESKTOP-CF9KSFJ",[[1,2909,"；看框位置"]],[2909,2909],[2914,2914]]],[1525157070756,["ww@DESKTOP-CF9KSFJ",[[-1,2904,";"]],[2905,2905],[2904,2904]]],[1525157071230,["ww@DESKTOP-CF9KSFJ",[[1,2904,"："]],[2904,2904],[2905,2905]]],[1525157155585,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525157155585,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525157145428,["ww@DESKTOP-CF9KSFJ",[[-1,2910,"看框位置"]],[2914,2914],[2910,2910]]],[1525157154538,["ww@DESKTOP-CF9KSFJ",[[1,2910,"特征提取以及方框位置"]],[2910,2910],[2920,2920]]],[1525157215587,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525157215587,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525157161230,["ww@DESKTOP-CF9KSFJ",[[1,2920,"的分类的计算（）"]],[2920,2920],[2928,2928]]],[1525157169456,["ww@DESKTOP-CF9KSFJ",[[1,2927,"框的位置，狂内部"]],[2927,2927],[2935,2935]]],[1525157171068,["ww@DESKTOP-CF9KSFJ",[[-1,2932,"狂内部"]],[2935,2935],[2932,2932]]],[1525157176248,["ww@DESKTOP-CF9KSFJ",[[1,2932,"框内部是什么"]],[2932,2932],[2938,2938]]],[1525157181301,["ww@DESKTOP-CF9KSFJ",[[1,2926,"。"]],[2926,2926],[2927,2927]]],[1525157189443,["ww@DESKTOP-CF9KSFJ",[[-1,2820,"\t"]],[2821,2821],[2820,2820]]],[1525157202069,["ww@DESKTOP-CF9KSFJ",[[1,2986,"（）"]],[2986,2986],[2988,2988]]],[1525157213639,["ww@DESKTOP-CF9KSFJ",[[1,2987,"You Only Look Once"]],[2987,2987],[3005,3005]]],[1525157249959,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525157249959,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525157222059,["ww@DESKTOP-CF9KSFJ",[[1,3017,"\n"]],[3006,3006],[3007,3007]]],[1525157369972,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525157369973,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525157322944,["ww@DESKTOP-CF9KSFJ",[[1,3008,"single shot方法"]],[3008,3008],[3021,3021]]],[1525157324875,["ww@DESKTOP-CF9KSFJ",[[1,3008,"\n"]],[3007,3007],[3008,3008]]],[1525157337561,["ww@DESKTOP-CF9KSFJ",[[1,3008,"region based 方法"]],[3008,3008],[3023,3023]]],[1525157338979,["ww@DESKTOP-CF9KSFJ",[[-1,3020," "]],[3021,3021],[3020,3020]]],[1525157359460,["ww@DESKTOP-CF9KSFJ",[[1,3008,"\n"]],[3008,3008],[3009,3009]]],[1525157359644,["ww@DESKTOP-CF9KSFJ",[[1,3009,"\n"]],[3009,3009],[3010,3010]]],[1525157359851,["ww@DESKTOP-CF9KSFJ",[[1,3010,"\n"]],[3010,3010],[3011,3011]]],[1525157549976,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525157549976,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525157535299,["ww@DESKTOP-CF9KSFJ",[[1,3049,"\n"]],[3039,3039],[3040,3040]]],[1525157535954,["ww@DESKTOP-CF9KSFJ",[[1,3050,"\n"]],[3040,3040],[3041,3041]]],[1525157538454,["ww@DESKTOP-CF9KSFJ",[[1,3041,"SSD"]],[3041,3041],[3044,3044]]],[1525157849979,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525157849979,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525157796804,["ww@DESKTOP-CF9KSFJ",[[1,3044,"（）"]],[3044,3044],[3046,3046]]],[1525157814428,["ww@DESKTOP-CF9KSFJ",[[1,3045,"single shot Multi-box Detector"]],[3045,3045],[3075,3075]]],[1525157817953,["ww@DESKTOP-CF9KSFJ",[[-1,3052,"s"]],[3053,3053],[3052,3052]]],[1525157818306,["ww@DESKTOP-CF9KSFJ",[[1,3052,"S"]],[3052,3052],[3053,3053]]],[1525157821001,["ww@DESKTOP-CF9KSFJ",[[-1,3045,"s"]],[3046,3046],[3045,3045]]],[1525157821483,["ww@DESKTOP-CF9KSFJ",[[1,3045,"S"]],[3045,3045],[3046,3046]]],[1525157826737,["ww@DESKTOP-CF9KSFJ",[[1,3086,"\n"]],[3076,3076],[3077,3077]]],[1525157909967,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525157909967,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525157879617,["ww@DESKTOP-CF9KSFJ",[[1,3087,"\n"]],[3077,3077],[3078,3078]]],[1525157892459,["ww@DESKTOP-CF9KSFJ",[[1,3078,"Two-Stage   Single SD"]],[3078,3078],[3099,3099]]],[1525157893489,["ww@DESKTOP-CF9KSFJ",[[-1,3098,"D"]],[3099,3099],[3098,3098]]],[1525157894794,["ww@DESKTOP-CF9KSFJ",[[1,3098,"hot"]],[3098,3098],[3101,3101]]],[1525157896801,["ww@DESKTOP-CF9KSFJ",[[1,3111,"\n"]],[3101,3101],[3102,3102]]],[1525158020040,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525158020040,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525157981401,["ww@DESKTOP-CF9KSFJ",[[1,3112,"\n"]],[3101,3101],[3102,3102]]],[1525157981593,["ww@DESKTOP-CF9KSFJ",[[1,3113,"\n"]],[3102,3102],[3103,3103]]],[1525157984595,["ww@DESKTOP-CF9KSFJ",[[1,3103,"mubiaojiance"]],[3103,3103],[3115,3115]]],[1525157986392,["ww@DESKTOP-CF9KSFJ",[[-1,3103,"mubiaojiance"]],[3115,3115],[3103,3103]]],[1525158001279,["ww@DESKTOP-CF9KSFJ",[[1,3103,"目标检测再进一步-动作捕捉"]],[3103,3103],[3116,3116]]],[1525158005253,["ww@DESKTOP-CF9KSFJ",[[1,3112,"行为"]],[3112,3112],[3114,3114]]],[1525158007744,["ww@DESKTOP-CF9KSFJ",[[-1,3111,"-"]],[3112,3112],[3111,3111]]],[1525158008402,["ww@DESKTOP-CF9KSFJ",[[1,3111,"-"]],[3111,3111],[3112,3112]]],[1525158010217,["ww@DESKTOP-CF9KSFJ",[[-1,3111,"-"]],[3112,3112],[3111,3111]]],[1525158015306,["ww@DESKTOP-CF9KSFJ",[[1,3111,"------"]],[3111,3111],[3117,3117]]],[1525158018090,["ww@DESKTOP-CF9KSFJ",[[1,3123,"。"]],[3123,3123],[3124,3124]]],[1525161440071,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525161440071,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525161401653,["ww@DESKTOP-CF9KSFJ",[[1,3134,"论文目录;"]],[3134,3134],[3139,3139]]],[1525161402204,["ww@DESKTOP-CF9KSFJ",[[1,3140,"\n"]],[3139,3139],[3140,3140]]],[1525161403077,["ww@DESKTOP-CF9KSFJ",[[-1,3140,"\n"]],[3140,3140],[3139,3139]]],[1525161403508,["ww@DESKTOP-CF9KSFJ",[[-1,3138,";"]],[3139,3139],[3138,3138]]],[1525161404488,["ww@DESKTOP-CF9KSFJ",[[1,3138,"："]],[3138,3138],[3139,3139]]],[1525161405179,["ww@DESKTOP-CF9KSFJ",[[1,3140,"\n"]],[3139,3139],[3140,3140]]],[1525161423560,["ww@DESKTOP-CF9KSFJ",[[1,3140,"作者：陈泰红\n链接：https://zhuanlan.zhihu.com/p/31117359\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n\n[2013-----RCNN----Rich feature hierarchies for accurate object detection and semantic segmentation](https://zhuanlan.zhihu.com/p/29936564)\n\n2013----Deep Neural Networks for Object Detection\n\n2014----SPPnet----Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-sppnet\n\n[2015----Fast R-CNN](https://zhuanlan.zhihu.com/p/29953111)\n\n[2016----Faster R-CNN-Towards Real-Time Object Detection with Region Proposal Networks](https://zhuanlan.zhihu.com/p/29969145)\n\n2016----Inside-Outside Net_Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n\n[2016----R-FCN-Object Detection via Region-based Fully Convolutional Networks](https://zhuanlan.zhihu.com/p/30788068)\n\n[2016----SSD-Single Shot MultiBox Detector](https://zhuanlan.zhihu.com/p/30478644)\n\n[目标检测之SSD代码分析（MXNet版）](https://zhuanlan.zhihu.com/p/30553929)\n\n2016----YOLO9000-better,faster,stronger\n\n2016----You Only Look Once-Unified, Real-Time Object Detection\n\n2017----A-Fast-RCNN_Hard Positive Generation via Adversary for Object Detection\n\n[2017----Deformable Convolutional Networks](https://zhuanlan.zhihu.com/p/30927896)\n\n2017----DSOD_ Learning Deeply Supervised Object Detectors from Scratch\n\n[2017----Focal Loss for Dense Object Detection](https://zhuanlan.zhihu.com/p/30701067)\n\n2017----Mask R-CNN\n\n2017----Speed_Accuracy trade-offs for modern convolutional object detectors\n\n[2017----Light-Head R-CNN_In Defense of Two-Stage Object Detector](https://zhuanlan.zhihu.com/p/31389174)"]],[3140,3140],[4717,4717]]],[1525161428299,["ww@DESKTOP-CF9KSFJ",[[-1,3140,"作者：陈泰红\n链接：https://zhuanlan.zhihu.com/p/31117359\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"]],[3140,3228],[3140,3140]]],[1525161430132,["ww@DESKTOP-CF9KSFJ",[[-1,3141,"\n"]],[3141,3141],[3140,3140]]],[1525161560073,[null,[[-1,1946,"）"],[1,1947,"（"]],[1946,1946],[1948,1948]]],[1525161560073,[null,[[1,1946,"）"],[-1,1946,"（"]],[1948,1948],[1946,1946]]],[1525161511123,["ww@DESKTOP-CF9KSFJ",[[1,4629,"\n"]],[4628,4628],[4629,4629]]],[1525161511514,["ww@DESKTOP-CF9KSFJ",[[1,4630,"\n"]],[4629,4629],[4630,4630]]],[1525161511898,["ww@DESKTOP-CF9KSFJ",[[1,4631,"\n"]],[4630,4630],[4631,4631]]],[1525161527548,["ww@DESKTOP-CF9KSFJ",[[1,3134,"## "]],[3134,3134],[3137,3137]]],[1525161540476,["ww@DESKTOP-CF9KSFJ",[[1,1929,"\n"]],[1928,1928],[1929,1929]]],[1525161546316,["ww@DESKTOP-CF9KSFJ",[[1,2018,"\n"]],[2017,2017],[2018,2018]]],[1525161680077,[null,[[-1,1947,"）"],[1,1948,"（"]],[1947,1947],[1949,1949]]],[1525161680077,[null,[[1,1947,"）"],[-1,1947,"（"]],[1949,1949],[1947,1947]]],[1525161658002,["ww@DESKTOP-CF9KSFJ",[[1,1901,"PASCAL VOC "]],[1901,1901],[1912,1912]]],[1525161666949,["ww@DESKTOP-CF9KSFJ",[[1,1900,"、ImageNet"]],[1900,1900],[1909,1909]]],[1525254263522,[null,[[-1,1967,"）"],[1,1968,"（"]],[1967,1967],[1969,1969]]],[1525254263523,[null,[[1,1967,"）"],[-1,1967,"（"]],[1969,1969],[1967,1967]]],[1525254225616,["ww@DESKTOP-CF9KSFJ",[[-1,1840,"```python\ndef \n```"]],[1840,1858],[1840,1840]]],[1525254226336,["ww@DESKTOP-CF9KSFJ",[[-1,1840,"\n"]],[1840,1840],[1839,1839]]],[1525254246832,["ww@DESKTOP-CF9KSFJ",[[-1,715,"CONV"],[1,719,"[link](conv"],[-1,797,"\n"],[-1,880,"\n"],[1,890,")"]],[715,890],[896,896]]],[1525254248642,["ww@DESKTOP-CF9KSFJ",[[1,715,"CONV"],[-1,715,"[link](conv"],[1,804,"\n"],[1,886,"\n"],[-1,895,")"]],[896,896],[715,890]]],[1525254803516,[null,[[-1,1948,"）"],[1,1949,"（"]],[1948,1948],[1950,1950]]],[1525254803516,[null,[[1,1948,"）"],[-1,1948,"（"]],[1950,1950],[1948,1948]]],[1525254773966,["ww@DESKTOP-CF9KSFJ",[[-1,2545,"5."]],[2547,2547],[2545,2545]]],[1525254863503,[null,[[-1,1948,"）"],[1,1949,"（"]],[1948,1948],[1950,1950]]],[1525254863503,[null,[[1,1948,"）"],[-1,1948,"（"]],[1950,1950],[1948,1948]]],[1525254837710,["ww@DESKTOP-CF9KSFJ",[[1,80,"\n"]],[79,79],[80,80]]],[1525255163522,[null,[[-1,1949,"）"],[1,1950,"（"]],[1949,1949],[1951,1951]]],[1525255163522,[null,[[1,1949,"）"],[-1,1949,"（"]],[1951,1951],[1949,1949]]],[1525255131613,["ww@DESKTOP-CF9KSFJ",[[-1,114,"（介绍，卷积神经网络发展）"]],[115,127],[114,114]]],[1525255134517,["ww@DESKTOP-CF9KSFJ",[[1,114,"  "]],[114,114],[116,116]]],[1525255403525,[null,[[-1,1938,"）"],[1,1939,"（"]],[1938,1938],[1940,1940]]],[1525255403525,[null,[[1,1938,"）"],[-1,1938,"（"]],[1940,1940],[1938,1938]]],[1525255400882,["ww@DESKTOP-CF9KSFJ",[[1,1830,"\n"]],[1829,1829],[1830,1830]]],[1525255401034,["ww@DESKTOP-CF9KSFJ",[[1,1831,"\n"]],[1830,1830],[1831,1831]]],[1525255402114,["ww@DESKTOP-CF9KSFJ",[[1,1832,"\n"]],[1830,1830],[1831,1831]]]]],["27457049-b3ed-4f4c-bbd3-6dd24fbdb1e8",1525255490962,"---\nstyle: plain\n---\n\nObject detection\n===========\n\n[[toc]]\n\n## 初步介绍\n卷积（边缘检测等等）\n\nPooling & ReLU（激活函数）\n下采样与池化\nVGG16  被很多目标检测用作特征提取\n* 16 weight layers\n* 13 convs + 3 fcs\n* 5 conv blocks\n```python\nINPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n`````````\n\n\n\n## Object detection\n\ndataset: PASCAL VOC 、ImageNet\nPASCAL VOC 图片信息相对imageNet更加复杂\n\n### 思路：\n\n#### 1.暴力解决：滑动窗口\n（已经猜到人脸大小设定窗口大小）\n![Lab-object-01]($res/Lab-object-01.png)\n穷举来实现，每个像素遍历\n\n#### 2.Region Proposal（区域提名）\n选择性搜索（颜色、纹理、尺度、包含关系）\ncolor\ntexture\nsize\nfill\n进行合并\n![Lab-object-02]($res/Lab-object-02.png)\n光晕部分由色泽不同和图像之间的相似度，对其进行合并，最终分成几个大块儿，对应到下面就是这个蓝色的块儿，就是猜测到的有可能有目标物体的图块儿。然后对候选的几个框进行分类，这样极大提高了速度，从穷举变成有限次的分类问题。\n类似层次聚类，本身是无监督的\n\n\n一张图片 以提名的方式提出大概有目标物体的提名，然后用分类的方法去看看属于哪一类\nSSD Driven Hierarchical clustering\n\n用卷积神经网络CNN，在这里就是用Region-based CNN （RCNN）\n![Lab-object-03]($res/Lab-object-03.png)\n流程:\n1.输入图像\n2.每张图像生成1K~2K个候选区域\n3.对每个候选区域，使用深度网络提取特征（AlexNet、VGG、CNN......）\n4.1将特征送入每一类的SVM分类器，判别是否属于该类\n4.2使用回归器精细修正候选框位置\n\npretrained with RCNN（用VGG16在imageNet）\n嫁接到Pascal上，不用输出1000类，输出21类即可（加一个不在考虑的类别）\n拿出卷积部分，再对最后的全连接层进行稍微改造即可\n\n对一张图要提名的太多2000，然后对每一个提名做卷积提取特征，也就是2000次，那么运行效率会比较慢\n引出Fast R-CNN\n\n##### Fast R-CNN\n一个图片经过卷积后的特征映射，信息量降低了，但对物体的分类的重要信息都保留下来了，所以只对特征映射做区域提名，进一步的卷积提取特征，不在原图上做区域提名，因此减少了计算量。\n\nselected search是在cpu上运行的，因此想到对区域提名部分再做一个卷积操作，使整个操作都能在GPU上完成。\n##### Faster RCNN\n两个分支：区域提名；特征提取以及方框位置的分类的计算。（框的位置，框内部是什么）\n一眼：看框打在哪儿。\n两眼：\n\n为了考虑实时性，达到在线预测的目的。\n\n##### YOLO（You Only Look Once）\n\n\n\n\nregion based方法\nsingle shot方法\n\nSSD（Single Shot Multi-box Detector）\n\nTwo-Stage   Single Shot\n\n目标检测再进一步------行为动作捕捉。\n\n\n\n\n\n\n\n\n\n## 论文目录：\n\n[2013-----RCNN----Rich feature hierarchies for accurate object detection and semantic segmentation](https://zhuanlan.zhihu.com/p/29936564)\n\n2013----Deep Neural Networks for Object Detection\n\n2014----SPPnet----Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-sppnet\n\n[2015----Fast R-CNN](https://zhuanlan.zhihu.com/p/29953111)\n\n[2016----Faster R-CNN-Towards Real-Time Object Detection with Region Proposal Networks](https://zhuanlan.zhihu.com/p/29969145)\n\n2016----Inside-Outside Net_Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n\n[2016----R-FCN-Object Detection via Region-based Fully Convolutional Networks](https://zhuanlan.zhihu.com/p/30788068)\n\n[2016----SSD-Single Shot MultiBox Detector](https://zhuanlan.zhihu.com/p/30478644)\n\n[目标检测之SSD代码分析（MXNet版）](https://zhuanlan.zhihu.com/p/30553929)\n\n2016----YOLO9000-better,faster,stronger\n\n2016----You Only Look Once-Unified, Real-Time Object Detection\n\n2017----A-Fast-RCNN_Hard Positive Generation via Adversary for Object Detection\n\n[2017----Deformable Convolutional Networks](https://zhuanlan.zhihu.com/p/30927896)\n\n2017----DSOD_ Learning Deeply Supervised Object Detectors from Scratch\n\n[2017----Focal Loss for Dense Object Detection](https://zhuanlan.zhihu.com/p/30701067)\n\n2017----Mask R-CNN\n\n2017----Speed_Accuracy trade-offs for modern convolutional object detectors\n\n[2017----Light-Head R-CNN_In Defense of Two-Stage Object Detector](https://zhuanlan.zhihu.com/p/31389174)\n\n\n\n[^1]: Nulla facilisi. Ut feugiat.\n[^2]: Nullam dui erat, malesuada eget viverra non. [malesuada](https://none)\n[^3]: Phasellus turpis odio, scelerisque eu purus ac\n",[[1525255440358,["ww@DESKTOP-CF9KSFJ",[[1,2042,"![20180502180245]($res/20180502180245.png)\n\n"]],[1830,1830],[2085,2085]]],[1525255447439,["ww@DESKTOP-CF9KSFJ",[[-1,2042,"![20180502180245]($res/20180502180245.png)"]],[2042,2084],[2042,2042]]],[1525255449899,["ww@DESKTOP-CF9KSFJ",[[1,1830,"![20180502180245]($res/20180502180245.png)"]],[1830,1830],[1872,1872]]],[1525255466989,["ww@DESKTOP-CF9KSFJ",[[1,81,"![20180502180257]($res/20180502180257.png)\n\n"]],[80,80],[124,124]]],[1525255507123,["ww@DESKTOP-CF9KSFJ",[[1,132,"\n"]],[132,132],[133,133]]],[1525255509514,["ww@DESKTOP-CF9KSFJ",[[-1,133," & "]],[136,136],[133,133]]],[1525255523710,["ww@DESKTOP-CF9KSFJ",[[1,132,"（）"]],[132,132],[134,134]]],[1525255526384,["ww@DESKTOP-CF9KSFJ",[[1,133,"压缩"]],[133,133],[135,135]]],[1525255530186,["ww@DESKTOP-CF9KSFJ",[[-1,142,"激活函数"]],[146,146],[142,142]]],[1525255592981,["ww@DESKTOP-CF9KSFJ",[[1,80,"特征提取"]],[80,80],[84,84]]],[1525255597284,["ww@DESKTOP-CF9KSFJ",[[1,80,"* "]],[80,80],[82,82]]],[1525255905674,["ww@DESKTOP-CF9KSFJ",[[-1,143,"ReLU（）"]],[143,149],[143,143]]],[1525255906092,["ww@DESKTOP-CF9KSFJ",[[-1,143,"\n"]],[143,143],[142,142]]],[1525255949057,["ww@DESKTOP-CF9KSFJ",[[-1,143,"下采样与池化"]],[143,149],[143,143]]],[1525255952844,["ww@DESKTOP-CF9KSFJ",[[1,142,"、"]],[142,142],[143,143]]],[1525255955105,["ww@DESKTOP-CF9KSFJ",[[-1,141,"）、"]],[143,143],[141,141]]],[1525255956306,["ww@DESKTOP-CF9KSFJ",[[1,141,"）"]],[141,141],[142,142]]],[1525255956866,["ww@DESKTOP-CF9KSFJ",[[1,144,"\n"]],[142,142],[143,143]]],[1525255959828,["ww@DESKTOP-CF9KSFJ",[[1,143,"上下采样"]],[143,143],[147,147]]],[1525255961129,["ww@DESKTOP-CF9KSFJ",[[-1,148,"\n"]],[148,148],[147,147]]],[1525256088585,["ww@DESKTOP-CF9KSFJ",[[1,80,"\n"]],[79,79],[80,80]]],[1525256089696,["ww@DESKTOP-CF9KSFJ",[[1,81,"\n"]],[80,80],[81,81]]]]],["eccc55b1-ccc4-4ed1-ac55-25080a62317f",1525256205820,"---\nstyle: plain\n---\n\nObject detection\n===========\n\n[[toc]]\n\n## 初步介绍\n卷积（边缘检测等等）\n\n\n* 特征提取\n![20180502180257]($res/20180502180257.png)\n\nPooling（压缩）\n上下采样\nVGG16  被很多目标检测用作特征提取\n* 16 weight layers\n* 13 convs + 3 fcs\n* 5 conv blocks\n```python\nINPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n`````````\n![20180502180245]($res/20180502180245.png)\n\n\n## Object detection\n\ndataset: PASCAL VOC 、ImageNet\nPASCAL VOC 图片信息相对imageNet更加复杂\n\n### 思路：\n\n#### 1.暴力解决：滑动窗口\n（已经猜到人脸大小设定窗口大小）\n![Lab-object-01]($res/Lab-object-01.png)\n穷举来实现，每个像素遍历\n\n#### 2.Region Proposal（区域提名）\n\n\n选择性搜索（颜色、纹理、尺度、包含关系）\ncolor\ntexture\nsize\nfill\n进行合并\n![Lab-object-02]($res/Lab-object-02.png)\n光晕部分由色泽不同和图像之间的相似度，对其进行合并，最终分成几个大块儿，对应到下面就是这个蓝色的块儿，就是猜测到的有可能有目标物体的图块儿。然后对候选的几个框进行分类，这样极大提高了速度，从穷举变成有限次的分类问题。\n类似层次聚类，本身是无监督的\n\n\n一张图片 以提名的方式提出大概有目标物体的提名，然后用分类的方法去看看属于哪一类\nSSD Driven Hierarchical clustering\n\n用卷积神经网络CNN，在这里就是用Region-based CNN （RCNN）\n![Lab-object-03]($res/Lab-object-03.png)\n流程:\n1.输入图像\n2.每张图像生成1K~2K个候选区域\n3.对每个候选区域，使用深度网络提取特征（AlexNet、VGG、CNN......）\n4.1将特征送入每一类的SVM分类器，判别是否属于该类\n4.2使用回归器精细修正候选框位置\n\npretrained with RCNN（用VGG16在imageNet）\n嫁接到Pascal上，不用输出1000类，输出21类即可（加一个不在考虑的类别）\n拿出卷积部分，再对最后的全连接层进行稍微改造即可\n\n对一张图要提名的太多2000，然后对每一个提名做卷积提取特征，也就是2000次，那么运行效率会比较慢\n引出Fast R-CNN\n\n##### Fast R-CNN\n一个图片经过卷积后的特征映射，信息量降低了，但对物体的分类的重要信息都保留下来了，所以只对特征映射做区域提名，进一步的卷积提取特征，不在原图上做区域提名，因此减少了计算量。\n\nselected search是在cpu上运行的，因此想到对区域提名部分再做一个卷积操作，使整个操作都能在GPU上完成。\n##### Faster RCNN\n两个分支：区域提名；特征提取以及方框位置的分类的计算。（框的位置，框内部是什么）\n一眼：看框打在哪儿。\n两眼：\n\n为了考虑实时性，达到在线预测的目的。\n\n##### YOLO（You Only Look Once）\n\n\n\n\nregion based方法\nsingle shot方法\n\nSSD（Single Shot Multi-box Detector）\n\nTwo-Stage   Single Shot\n\n目标检测再进一步------行为动作捕捉。\n\n\n\n\n\n\n\n\n\n## 论文目录：\n\n[2013-----RCNN----Rich feature hierarchies for accurate object detection and semantic segmentation](https://zhuanlan.zhihu.com/p/29936564)\n\n2013----Deep Neural Networks for Object Detection\n\n2014----SPPnet----Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-sppnet\n\n[2015----Fast R-CNN](https://zhuanlan.zhihu.com/p/29953111)\n\n[2016----Faster R-CNN-Towards Real-Time Object Detection with Region Proposal Networks](https://zhuanlan.zhihu.com/p/29969145)\n\n2016----Inside-Outside Net_Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n\n[2016----R-FCN-Object Detection via Region-based Fully Convolutional Networks](https://zhuanlan.zhihu.com/p/30788068)\n\n[2016----SSD-Single Shot MultiBox Detector](https://zhuanlan.zhihu.com/p/30478644)\n\n[目标检测之SSD代码分析（MXNet版）](https://zhuanlan.zhihu.com/p/30553929)\n\n2016----YOLO9000-better,faster,stronger\n\n2016----You Only Look Once-Unified, Real-Time Object Detection\n\n2017----A-Fast-RCNN_Hard Positive Generation via Adversary for Object Detection\n\n[2017----Deformable Convolutional Networks](https://zhuanlan.zhihu.com/p/30927896)\n\n2017----DSOD_ Learning Deeply Supervised Object Detectors from Scratch\n\n[2017----Focal Loss for Dense Object Detection](https://zhuanlan.zhihu.com/p/30701067)\n\n2017----Mask R-CNN\n\n2017----Speed_Accuracy trade-offs for modern convolutional object detectors\n\n[2017----Light-Head R-CNN_In Defense of Two-Stage Object Detector](https://zhuanlan.zhihu.com/p/31389174)\n\n\n\n[^1]: Nulla facilisi. Ut feugiat.\n[^2]: Nullam dui erat, malesuada eget viverra non. [malesuada](https://none)\n[^3]: Phasellus turpis odio, scelerisque eu purus ac\n",[[1525256151653,["ww@DESKTOP-CF9KSFJ",[[1,225,"![20170325211712248]($res/20170325211712248.gif)\n\n"]],[80,80],[274,274]]],[1525256158120,["ww@DESKTOP-CF9KSFJ",[[-1,225,"![20170325211712248]($res/20170325211712248.gif)"]],[225,273],[225,225]]],[1525256160000,["ww@DESKTOP-CF9KSFJ",[[1,80,"![20170325211712248]($res/20170325211712248.gif)"]],[80,80],[128,128]]]]],["a73a8e68-016a-4c31-b9fb-950ea42eddc1",1525256353940,"---\nstyle: plain\n---\n\nObject detection\n===========\n\n[[toc]]\n\n## 初步介绍\n卷积（边缘检测等等）\n![20170325211712248]($res/20170325211712248.gif)\n\n* 特征提取\n![20180502180257]($res/20180502180257.png)\n\nPooling（压缩）\n上下采样\nVGG16  被很多目标检测用作特征提取\n* 16 weight layers\n* 13 convs + 3 fcs\n* 5 conv blocks\n\n\n```python\nINPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n`````````\n![20180502180245]($res/20180502180245.png)\n\n\n## Object detection\n\ndataset: PASCAL VOC 、ImageNet\nPASCAL VOC 图片信息相对imageNet更加复杂\n\n### 思路：\n\n#### 1.暴力解决：滑动窗口\n（已经猜到人脸大小设定窗口大小）\n![Lab-object-01]($res/Lab-object-01.png)\n穷举来实现，每个像素遍历\n\n#### 2.Region Proposal（区域提名）\n\n\n选择性搜索（颜色、纹理、尺度、包含关系）\ncolor\ntexture\nsize\nfill\n进行合并\n![Lab-object-02]($res/Lab-object-02.png)\n光晕部分由色泽不同和图像之间的相似度，对其进行合并，最终分成几个大块儿，对应到下面就是这个蓝色的块儿，就是猜测到的有可能有目标物体的图块儿。然后对候选的几个框进行分类，这样极大提高了速度，从穷举变成有限次的分类问题。\n类似层次聚类，本身是无监督的\n\n\n一张图片 以提名的方式提出大概有目标物体的提名，然后用分类的方法去看看属于哪一类\nSSD Driven Hierarchical clustering\n\n用卷积神经网络CNN，在这里就是用Region-based CNN （RCNN）\n![Lab-object-03]($res/Lab-object-03.png)\n流程:\n1.输入图像\n2.每张图像生成1K~2K个候选区域\n3.对每个候选区域，使用深度网络提取特征（AlexNet、VGG、CNN......）\n4.1将特征送入每一类的SVM分类器，判别是否属于该类\n4.2使用回归器精细修正候选框位置\n\npretrained with RCNN（用VGG16在imageNet）\n嫁接到Pascal上，不用输出1000类，输出21类即可（加一个不在考虑的类别）\n拿出卷积部分，再对最后的全连接层进行稍微改造即可\n\n对一张图要提名的太多2000，然后对每一个提名做卷积提取特征，也就是2000次，那么运行效率会比较慢\n引出Fast R-CNN\n\n##### Fast R-CNN\n一个图片经过卷积后的特征映射，信息量降低了，但对物体的分类的重要信息都保留下来了，所以只对特征映射做区域提名，进一步的卷积提取特征，不在原图上做区域提名，因此减少了计算量。\n\nselected search是在cpu上运行的，因此想到对区域提名部分再做一个卷积操作，使整个操作都能在GPU上完成。\n##### Faster RCNN\n两个分支：区域提名；特征提取以及方框位置的分类的计算。（框的位置，框内部是什么）\n一眼：看框打在哪儿。\n两眼：\n\n为了考虑实时性，达到在线预测的目的。\n\n##### YOLO（You Only Look Once）\n\n\n\n\nregion based方法\nsingle shot方法\n\nSSD（Single Shot Multi-box Detector）\n\nTwo-Stage   Single Shot\n\n目标检测再进一步------行为动作捕捉。\n\n\n\n\n\n\n\n\n\n## 论文目录：\n\n[2013-----RCNN----Rich feature hierarchies for accurate object detection and semantic segmentation](https://zhuanlan.zhihu.com/p/29936564)\n\n2013----Deep Neural Networks for Object Detection\n\n2014----SPPnet----Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-sppnet\n\n[2015----Fast R-CNN](https://zhuanlan.zhihu.com/p/29953111)\n\n[2016----Faster R-CNN-Towards Real-Time Object Detection with Region Proposal Networks](https://zhuanlan.zhihu.com/p/29969145)\n\n2016----Inside-Outside Net_Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n\n[2016----R-FCN-Object Detection via Region-based Fully Convolutional Networks](https://zhuanlan.zhihu.com/p/30788068)\n\n[2016----SSD-Single Shot MultiBox Detector](https://zhuanlan.zhihu.com/p/30478644)\n\n[目标检测之SSD代码分析（MXNet版）](https://zhuanlan.zhihu.com/p/30553929)\n\n2016----YOLO9000-better,faster,stronger\n\n2016----You Only Look Once-Unified, Real-Time Object Detection\n\n2017----A-Fast-RCNN_Hard Positive Generation via Adversary for Object Detection\n\n[2017----Deformable Convolutional Networks](https://zhuanlan.zhihu.com/p/30927896)\n\n2017----DSOD_ Learning Deeply Supervised Object Detectors from Scratch\n\n[2017----Focal Loss for Dense Object Detection](https://zhuanlan.zhihu.com/p/30701067)\n\n2017----Mask R-CNN\n\n2017----Speed_Accuracy trade-offs for modern convolutional object detectors\n\n[2017----Light-Head R-CNN_In Defense of Two-Stage Object Detector](https://zhuanlan.zhihu.com/p/31389174)\n\n\n\n[^1]: Nulla facilisi. Ut feugiat.\n[^2]: Nullam dui erat, malesuada eget viverra non. [malesuada](https://none)\n[^3]: Phasellus turpis odio, scelerisque eu purus ac\n",[[1525256315482,["ww@DESKTOP-CF9KSFJ",[[1,193,"\n"]],[192,192],[193,193]]],[1525256319803,["ww@DESKTOP-CF9KSFJ",[[1,880,"![20170325211641810]($res/20170325211641810.gif)\n\n"]],[193,193],[929,929]]],[1525256340598,["ww@DESKTOP-CF9KSFJ",[[1,194,"![20170325211641810]($res/20170325211641810.gif)\n\n"]],[193,193],[243,243]]],[1525256345727,["ww@DESKTOP-CF9KSFJ",[[-1,193,"\n"]],[193,193],[192,192]]],[1525256361625,["ww@DESKTOP-CF9KSFJ",[[1,181,"** "]],[181,181],[184,184]]],[1525256364098,["ww@DESKTOP-CF9KSFJ",[[1,195,"**"]],[195,195],[197,197]]],[1525256366512,["ww@DESKTOP-CF9KSFJ",[[-1,194,"）"]],[195,195],[194,194]]],[1525256370137,["ww@DESKTOP-CF9KSFJ",[[-1,183," "]],[184,184],[183,183]]],[1525256374410,["ww@DESKTOP-CF9KSFJ",[[1,193,"）"]],[193,193],[194,194]]],[1525256446601,["ww@DESKTOP-CF9KSFJ",[[1,247,"\n"]],[246,246],[247,247]]],[1525256455387,["ww@DESKTOP-CF9KSFJ",[[1,247,"** Padding **"]],[247,247],[260,260]]],[1525256456384,["ww@DESKTOP-CF9KSFJ",[[1,261,"\n"]],[260,260],[261,261]]]]],["068beb20-3315-4a4a-8fd9-6870deffaf9d",1525256587619,"---\nstyle: plain\n---\n\nObject detection\n===========\n\n[[toc]]\n\n## 初步介绍\n卷积（边缘检测等等）\n![20170325211712248]($res/20170325211712248.gif)\n\n* 特征提取\n![20180502180257]($res/20180502180257.png)\n\n**Pooling（压缩）**\n![20170325211641810]($res/20170325211641810.gif)\n\n** Padding **\n\n上下采样\nVGG16  被很多目标检测用作特征提取\n* 16 weight layers\n* 13 convs + 3 fcs\n* 5 conv blocks\n\n\n```python\nINPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\n![20170325211641810]($res/20170325211641810.gif)\n\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n`````````\n![20180502180245]($res/20180502180245.png)\n\n\n## Object detection\n\ndataset: PASCAL VOC 、ImageNet\nPASCAL VOC 图片信息相对imageNet更加复杂\n\n### 思路：\n\n#### 1.暴力解决：滑动窗口\n（已经猜到人脸大小设定窗口大小）\n![Lab-object-01]($res/Lab-object-01.png)\n穷举来实现，每个像素遍历\n\n#### 2.Region Proposal（区域提名）\n\n\n选择性搜索（颜色、纹理、尺度、包含关系）\ncolor\ntexture\nsize\nfill\n进行合并\n![Lab-object-02]($res/Lab-object-02.png)\n光晕部分由色泽不同和图像之间的相似度，对其进行合并，最终分成几个大块儿，对应到下面就是这个蓝色的块儿，就是猜测到的有可能有目标物体的图块儿。然后对候选的几个框进行分类，这样极大提高了速度，从穷举变成有限次的分类问题。\n类似层次聚类，本身是无监督的\n\n\n一张图片 以提名的方式提出大概有目标物体的提名，然后用分类的方法去看看属于哪一类\nSSD Driven Hierarchical clustering\n\n用卷积神经网络CNN，在这里就是用Region-based CNN （RCNN）\n![Lab-object-03]($res/Lab-object-03.png)\n流程:\n1.输入图像\n2.每张图像生成1K~2K个候选区域\n3.对每个候选区域，使用深度网络提取特征（AlexNet、VGG、CNN......）\n4.1将特征送入每一类的SVM分类器，判别是否属于该类\n4.2使用回归器精细修正候选框位置\n\npretrained with RCNN（用VGG16在imageNet）\n嫁接到Pascal上，不用输出1000类，输出21类即可（加一个不在考虑的类别）\n拿出卷积部分，再对最后的全连接层进行稍微改造即可\n\n对一张图要提名的太多2000，然后对每一个提名做卷积提取特征，也就是2000次，那么运行效率会比较慢\n引出Fast R-CNN\n\n##### Fast R-CNN\n一个图片经过卷积后的特征映射，信息量降低了，但对物体的分类的重要信息都保留下来了，所以只对特征映射做区域提名，进一步的卷积提取特征，不在原图上做区域提名，因此减少了计算量。\n\nselected search是在cpu上运行的，因此想到对区域提名部分再做一个卷积操作，使整个操作都能在GPU上完成。\n##### Faster RCNN\n两个分支：区域提名；特征提取以及方框位置的分类的计算。（框的位置，框内部是什么）\n一眼：看框打在哪儿。\n两眼：\n\n为了考虑实时性，达到在线预测的目的。\n\n##### YOLO（You Only Look Once）\n\n\n\n\nregion based方法\nsingle shot方法\n\nSSD（Single Shot Multi-box Detector）\n\nTwo-Stage   Single Shot\n\n目标检测再进一步------行为动作捕捉。\n\n\n\n\n\n\n\n\n\n## 论文目录：\n\n[2013-----RCNN----Rich feature hierarchies for accurate object detection and semantic segmentation](https://zhuanlan.zhihu.com/p/29936564)\n\n2013----Deep Neural Networks for Object Detection\n\n2014----SPPnet----Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-sppnet\n\n[2015----Fast R-CNN](https://zhuanlan.zhihu.com/p/29953111)\n\n[2016----Faster R-CNN-Towards Real-Time Object Detection with Region Proposal Networks](https://zhuanlan.zhihu.com/p/29969145)\n\n2016----Inside-Outside Net_Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n\n[2016----R-FCN-Object Detection via Region-based Fully Convolutional Networks](https://zhuanlan.zhihu.com/p/30788068)\n\n[2016----SSD-Single Shot MultiBox Detector](https://zhuanlan.zhihu.com/p/30478644)\n\n[目标检测之SSD代码分析（MXNet版）](https://zhuanlan.zhihu.com/p/30553929)\n\n2016----YOLO9000-better,faster,stronger\n\n2016----You Only Look Once-Unified, Real-Time Object Detection\n\n2017----A-Fast-RCNN_Hard Positive Generation via Adversary for Object Detection\n\n[2017----Deformable Convolutional Networks](https://zhuanlan.zhihu.com/p/30927896)\n\n2017----DSOD_ Learning Deeply Supervised Object Detectors from Scratch\n\n[2017----Focal Loss for Dense Object Detection](https://zhuanlan.zhihu.com/p/30701067)\n\n2017----Mask R-CNN\n\n2017----Speed_Accuracy trade-offs for modern convolutional object detectors\n\n[2017----Light-Head R-CNN_In Defense of Two-Stage Object Detector](https://zhuanlan.zhihu.com/p/31389174)\n\n\n\n[^1]: Nulla facilisi. Ut feugiat.\n[^2]: Nullam dui erat, malesuada eget viverra non. [malesuada](https://none)\n[^3]: Phasellus turpis odio, scelerisque eu purus ac\n",[[1525256533312,["ww@DESKTOP-CF9KSFJ",[[1,262,"![20170325220130828]($res/20170325220130828.gif)\n\n"]],[261,261],[311,311]]],[1525256541975,["ww@DESKTOP-CF9KSFJ",[[-1,257," "]],[258,258],[257,257]]],[1525256544008,["ww@DESKTOP-CF9KSFJ",[[-1,249," "]],[250,250],[249,249]]],[1525256623936,["ww@DESKTOP-CF9KSFJ",[[1,259,"外面加一圈0."]],[259,259],[266,266]]],[1525256627039,["ww@DESKTOP-CF9KSFJ",[[-1,260,"面加一圈"]],[264,264],[260,260]]],[1525256629134,["ww@DESKTOP-CF9KSFJ",[[-1,259,"外0."]],[262,262],[259,259]]],[1525256629886,["ww@DESKTOP-CF9KSFJ",[[-1,259,"\n"]],[259,259],[258,258]]],[1525257582558,["ww@DESKTOP-CF9KSFJ",[[1,2719,"**"]],[2719,2719],[2721,2721]]],[1525257585005,["ww@DESKTOP-CF9KSFJ",[[1,2724,"**"]],[2724,2724],[2726,2726]]],[1525257649550,["ww@DESKTOP-CF9KSFJ",[[1,3253,"看框内是什么"]],[3253,3253],[3259,3259]]],[1525257703286,["ww@DESKTOP-CF9KSFJ",[[1,3312,"整张图像输入，直接输出回归层Bounding Box"]],[3312,3312],[3338,3338]]],[1525257705924,["ww@DESKTOP-CF9KSFJ",[[1,3326," "]],[3326,3326],[3327,3327]]],[1525257720493,["ww@DESKTOP-CF9KSFJ",[[1,3339,"的位置和Bounding Box的"]],[3339,3339],[3356,3356]]],[1525257721203,["ww@DESKTOP-CF9KSFJ",[[-1,3355,"的"]],[3356,3356],[3355,3355]]],[1525257724574,["ww@DESKTOP-CF9KSFJ",[[1,3355,"所属的类别"]],[3355,3355],[3360,3360]]],[1525257775604,["ww@DESKTOP-CF9KSFJ",[[1,3362,"缺点：当被检测目标集中在某个区域时效果很差，"]],[3362,3362],[3384,3384]]],[1525257780483,["ww@DESKTOP-CF9KSFJ",[[-1,3383,"，"]],[3384,3384],[3383,3383]]],[1525257781310,["ww@DESKTOP-CF9KSFJ",[[1,3383,"。"]],[3383,3383],[3384,3384]]],[1525257786290,["ww@DESKTOP-CF9KSFJ",[[1,3362,"\n"]],[3361,3361],[3362,3362]]],[1525257787890,["ww@DESKTOP-CF9KSFJ",[[1,3363,"\n"]],[3361,3361],[3362,3362]]],[1525257806368,["ww@DESKTOP-CF9KSFJ",[[1,3362,"将一幅图像分成SxS个网格(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object。"]],[3362,3362],[3427,3427]]],[1525257823509,["ww@DESKTOP-CF9KSFJ",[[1,3451,"因为每个格子只能对应预测一个bounding box和类别概率，因此无法预测邻近区域的物体。检测精度下降了。"]],[3451,3451],[3505,3505]]],[1525257825898,["ww@DESKTOP-CF9KSFJ",[[-1,3496,"。"]],[3497,3497],[3496,3496]]],[1525257826822,["ww@DESKTOP-CF9KSFJ",[[1,3496,"。"]],[3496,3496],[3497,3497]]],[1525257827393,["ww@DESKTOP-CF9KSFJ",[[-1,3496,"。"]],[3497,3497],[3496,3496]]],[1525257828219,["ww@DESKTOP-CF9KSFJ",[[1,3496,"，"]],[3496,3496],[3497,3497]]],[1525257836675,["ww@DESKTOP-CF9KSFJ",[[-1,3507,"region based方法\nsingle shot方法"]],[3507,3535],[3507,3507]]],[1525257837098,["ww@DESKTOP-CF9KSFJ",[[-1,3508,"\n"]],[3507,3507],[3506,3506]]],[1525257838115,["ww@DESKTOP-CF9KSFJ",[[-1,3507,"\n"]],[3507,3507],[3506,3506]]],[1525257850772,["ww@DESKTOP-CF9KSFJ",[[1,3544,"\n"]],[3543,3543],[3544,3544]]],[1525257854605,["ww@DESKTOP-CF9KSFJ",[[-1,3544,"\n"]],[3544,3544],[3543,3543]]],[1525257854860,["ww@DESKTOP-CF9KSFJ",[[1,3507,"\n"]],[3506,3506],[3507,3507]]],[1525257855316,["ww@DESKTOP-CF9KSFJ",[[1,3508,"\n"]],[3506,3506],[3507,3507]]],[1525257855708,["ww@DESKTOP-CF9KSFJ",[[1,3507,"region based方法\nsingle shot方法"]],[3507,3507],[3507,3535]]],[1525257858443,["ww@DESKTOP-CF9KSFJ",[[1,3574,"\n"]],[3573,3573],[3574,3574]]],[1525257875052,["ww@DESKTOP-CF9KSFJ",[[1,3573,"predict object（物体），以及其 归属类别的 score（得分）；同时，在 feature map 上使用小的卷积核，去 predict 一系列 bounding boxes 的 box offsets。"]],[3573,3573],[3681,3681]]],[1525257916225,["ww@DESKTOP-CF9KSFJ",[[1,3683,"\n"]],[3682,3682],[3683,3683]]],[1525258132281,["ww@DESKTOP-CF9KSFJ",[[-1,3017,"##"]],[3019,3019],[3017,3017]]],[1525258143931,["ww@DESKTOP-CF9KSFJ",[[-1,2226,"#"]],[2227,2227],[2226,2226]]],[1525258147857,["ww@DESKTOP-CF9KSFJ",[[-1,2314,"#"]],[2315,2315],[2314,2314]]],[1525258156776,["ww@DESKTOP-CF9KSFJ",[[-1,3179,"##"]],[3181,3181],[3179,3179]]],[1525258159952,["ww@DESKTOP-CF9KSFJ",[[-1,3278,"##"]],[3280,3280],[3278,3278]]],[1525258191393,["ww@DESKTOP-CF9KSFJ",[[-1,3499,"region based方法\nsingle shot方法"]],[3499,3527],[3499,3499]]],[1525258192032,["ww@DESKTOP-CF9KSFJ",[[-1,3500,"\n"]],[3499,3499],[3498,3498]]],[1525258194072,["ww@DESKTOP-CF9KSFJ",[[-1,3499,"\n"]],[3499,3499],[3498,3498]]],[1525258210409,["ww@DESKTOP-CF9KSFJ",[[-1,3646,"Two-Stage   Single Shot"]],[3646,3669],[3646,3646]]],[1525258210880,["ww@DESKTOP-CF9KSFJ",[[-1,3647,"\n"]],[3646,3646],[3645,3645]]],[1525258211352,["ww@DESKTOP-CF9KSFJ",[[-1,3646,"\n"]],[3645,3645],[3644,3644]]],[1525258212592,["ww@DESKTOP-CF9KSFJ",[[-1,3645,"\n"]],[3645,3645],[3644,3644]]],[1525258216545,["ww@DESKTOP-CF9KSFJ",[[-1,3675,"\n"]],[3674,3674],[3673,3673]]],[1525258216744,["ww@DESKTOP-CF9KSFJ",[[-1,3674,"\n"]],[3673,3673],[3672,3672]]],[1525258216905,["ww@DESKTOP-CF9KSFJ",[[-1,3673,"\n"]],[3672,3672],[3671,3671]]],[1525258217128,["ww@DESKTOP-CF9KSFJ",[[-1,3672,"\n"]],[3671,3671],[3670,3670]]],[1525258356520,["ww@DESKTOP-CF9KSFJ",[[1,2635,"\n"]],[2634,2634],[2635,2635]]],[1525258362691,["ww@DESKTOP-CF9KSFJ",[[1,2635,"### RCNN"]],[2635,2635],[2643,2643]]],[1525258372791,["ww@DESKTOP-CF9KSFJ",[[1,2726,"\n"]],[2725,2725],[2726,2726]]],[1525258373226,["ww@DESKTOP-CF9KSFJ",[[1,2727,"\n"]],[2726,2726],[2727,2727]]],[1525258374216,["ww@DESKTOP-CF9KSFJ",[[1,2728,"\n"]],[2726,2726],[2727,2727]]],[1525258378058,["ww@DESKTOP-CF9KSFJ",[[1,2727,"创新点："]],[2727,2727],[2731,2731]]],[1525258378727,["ww@DESKTOP-CF9KSFJ",[[1,2733,"\n"]],[2731,2731],[2732,2732]]],[1525258385113,["ww@DESKTOP-CF9KSFJ",[[1,2732,"*"]],[2732,2732],[2733,2733]]],[1525258386047,["ww@DESKTOP-CF9KSFJ",[[1,2735,"\n"]],[2733,2733],[2734,2734]]],[1525258387378,["ww@DESKTOP-CF9KSFJ",[[1,2734,"* "]],[2734,2734],[2736,2736]]],[1525258396697,["ww@DESKTOP-CF9KSFJ",[[1,2733," 采用CNN网络提取图像特征，从经验驱动的人造特征范式HOG、SIFT到数据驱动的表示学习范式，提高特征对样本的表示能力；"]],[2733,2733],[2794,2794]]],[1525258399105,["ww@DESKTOP-CF9KSFJ",[[1,2785,"   "]],[2785,2785],[2788,2788]]],[1525258413587,["ww@DESKTOP-CF9KSFJ",[[1,2800,"采用大样本下有监督预训练+小样本微调的方式解决小样本难以训练甚至过拟合等问题。"]],[2800,2800],[2839,2839]]],[1525258430048,["ww@DESKTOP-CF9KSFJ",[[1,2841,"\n"]],[2840,2840],[2841,2841]]],[1525258430192,["ww@DESKTOP-CF9KSFJ",[[1,2842,"\n"]],[2841,2841],[2842,2842]]],[1525258433121,["ww@DESKTOP-CF9KSFJ",[[1,2841,"问题："]],[2841,2841],[2844,2844]]],[1525258434191,["ww@DESKTOP-CF9KSFJ",[[1,2846,"\n"]],[2844,2844],[2845,2845]]],[1525258436754,["ww@DESKTOP-CF9KSFJ",[[1,2845,"* "]],[2845,2845],[2847,2847]]],[1525258438913,["ww@DESKTOP-CF9KSFJ",[[1,2848,"* "]],[2848,2848],[2850,2850]]],[1525258450659,["ww@DESKTOP-CF9KSFJ",[[1,2847,"近10年以来，以人工经验特征为主导的物体检测任务mAP【物体类别和位置的平均精度】提升缓慢"]],[2847,2847],[2892,2892]]],[1525258465446,["ww@DESKTOP-CF9KSFJ",[[1,2895,"随着ReLu激励函数、dropout正则化手段和大规模图像样本集ILSVRC的出现，在2012年ImageNet大规模视觉识别挑战赛中，Hinton及他的学生采用CNN特征获得了最高的图像识别精确度；"]],[2895,2895],[2995,2995]]],[1525258467495,["ww@DESKTOP-CF9KSFJ",[[1,2996,"\n"]],[2995,2995],[2996,2996]]],[1525258477719,["ww@DESKTOP-CF9KSFJ",[[1,2996,"* 上述比赛后，引发了一股“是否可以采用CNN特征来提高当前一直停滞不前的物体检测准确率“的热潮。"]],[2996,2996],[3045,3045]]],[1525258479847,["ww@DESKTOP-CF9KSFJ",[[1,3046,"\n"]],[3045,3045],[3046,3046]]],[1525258479999,["ww@DESKTOP-CF9KSFJ",[[1,3047,"\n"]],[3046,3046],[3047,3047]]],[1525258483888,["ww@DESKTOP-CF9KSFJ",[[1,2841,"**"]],[2841,2841],[2843,2843]]],[1525258486280,["ww@DESKTOP-CF9KSFJ",[[1,2846,"**"]],[2846,2846],[2848,2848]]],[1525258495241,["ww@DESKTOP-CF9KSFJ",[[1,3351,"**"]],[3351,3351],[3353,3353]]],[1525258498456,["ww@DESKTOP-CF9KSFJ",[[1,3363,"**"]],[3363,3363],[3365,3365]]],[1525258507367,["ww@DESKTOP-CF9KSFJ",[[-1,3351,"**"]],[3353,3353],[3351,3351]]],[1525258508343,["ww@DESKTOP-CF9KSFJ",[[-1,3361,"**"]],[3363,3363],[3361,3361]]],[1525258617066,["ww@DESKTOP-CF9KSFJ",[[1,3345,"。"]],[3345,3345],[3346,3346]]],[1525258617983,["ww@DESKTOP-CF9KSFJ",[[1,3348,"\n"]],[3346,3346],[3347,3347]]],[1525258618447,["ww@DESKTOP-CF9KSFJ",[[1,3349,"\n"]],[3347,3347],[3348,3348]]],[1525258636672,["ww@DESKTOP-CF9KSFJ",[[1,3282,"\n"]],[3281,3281],[3282,3282]]],[1525258636823,["ww@DESKTOP-CF9KSFJ",[[1,3283,"\n"]],[3282,3282],[3283,3283]]],[1525258636983,["ww@DESKTOP-CF9KSFJ",[[1,3284,"\n"]],[3283,3283],[3284,3284]]],[1525258638351,["ww@DESKTOP-CF9KSFJ",[[1,3285,"\n"]],[3282,3282],[3283,3283]]],[1525258638503,["ww@DESKTOP-CF9KSFJ",[[1,3286,"\n"]],[3283,3283],[3284,3284]]],[1525258639879,["ww@DESKTOP-CF9KSFJ",[[1,3287,"\n"]],[3282,3282],[3283,3283]]],[1525258640038,["ww@DESKTOP-CF9KSFJ",[[1,3288,"\n"]],[3283,3283],[3284,3284]]],[1525258642320,["ww@DESKTOP-CF9KSFJ",[[1,3289,"\n"]],[3282,3282],[3283,3283]]],[1525258784543,["ww@DESKTOP-CF9KSFJ",[[1,309,"\n"]],[308,308],[309,309]]],[1525258786038,["ww@DESKTOP-CF9KSFJ",[[1,309,"**重叠度（IOU）:**"]],[309,309],[322,322]]],[1525258789143,["ww@DESKTOP-CF9KSFJ",[[1,323,"\n"]],[322,322],[323,323]]],[1525258789846,["ww@DESKTOP-CF9KSFJ",[[1,324,"\n"]],[323,323],[324,324]]],[1525258797686,["ww@DESKTOP-CF9KSFJ",[[1,323,"物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。"]],[323,323],[412,412]]],[1525258799390,["ww@DESKTOP-CF9KSFJ",[[1,414,"\n"]],[412,412],[413,413]]]]],["255721de-65f0-486b-b20f-d8cd9a2b1712",1525258880439,"---\nstyle: plain\n---\n\nObject detection\n===========\n\n[[toc]]\n\n## 初步介绍\n卷积（边缘检测等等）\n![20170325211712248]($res/20170325211712248.gif)\n\n* 特征提取\n![20180502180257]($res/20180502180257.png)\n\n**Pooling（压缩）**\n![20170325211641810]($res/20170325211641810.gif)\n\n**Padding**\n![20170325220130828]($res/20170325220130828.gif)\n\n**重叠度（IOU）:**\n物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。\n\n\n上下采样\nVGG16  被很多目标检测用作特征提取\n* 16 weight layers\n* 13 convs + 3 fcs\n* 5 conv blocks\n\n\n```python\nINPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\n![20170325211641810]($res/20170325211641810.gif)\n\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n`````````\n![20180502180245]($res/20180502180245.png)\n\n\n## Object detection\n\ndataset: PASCAL VOC 、ImageNet\nPASCAL VOC 图片信息相对imageNet更加复杂\n\n### 思路：\n\n### 1.暴力解决：滑动窗口\n（已经猜到人脸大小设定窗口大小）\n![Lab-object-01]($res/Lab-object-01.png)\n穷举来实现，每个像素遍历\n\n### 2.Region Proposal（区域提名）\n\n\n选择性搜索（颜色、纹理、尺度、包含关系）\ncolor\ntexture\nsize\nfill\n进行合并\n![Lab-object-02]($res/Lab-object-02.png)\n光晕部分由色泽不同和图像之间的相似度，对其进行合并，最终分成几个大块儿，对应到下面就是这个蓝色的块儿，就是猜测到的有可能有目标物体的图块儿。然后对候选的几个框进行分类，这样极大提高了速度，从穷举变成有限次的分类问题。\n类似层次聚类，本身是无监督的\n\n\n一张图片 以提名的方式提出大概有目标物体的提名，然后用分类的方法去看看属于哪一类\nSSD Driven Hierarchical clustering\n\n### RCNN\n用卷积神经网络CNN，在这里就是用Region-based CNN （RCNN）\n![Lab-object-03]($res/Lab-object-03.png)\n\n创新点：\n* 采用CNN网络提取图像特征，从经验驱动的人造特征范式HOG、SIFT到数据驱动的表示学习范式，提高特征   对样本的表示能力；\n* 采用大样本下有监督预训练+小样本微调的方式解决小样本难以训练甚至过拟合等问题。\n\n**问题：**\n* 近10年以来，以人工经验特征为主导的物体检测任务mAP【物体类别和位置的平均精度】提升缓慢\n* 随着ReLu激励函数、dropout正则化手段和大规模图像样本集ILSVRC的出现，在2012年ImageNet大规模视觉识别挑战赛中，Hinton及他的学生采用CNN特征获得了最高的图像识别精确度；\n* 上述比赛后，引发了一股“是否可以采用CNN特征来提高当前一直停滞不前的物体检测准确率“的热潮。\n\n\n**流程:**\n1.输入图像\n2.每张图像生成1K~2K个候选区域\n3.对每个候选区域，使用深度网络提取特征（AlexNet、VGG、CNN......）\n4.1将特征送入每一类的SVM分类器，判别是否属于该类\n4.2使用回归器精细修正候选框位置\n\npretrained with RCNN（用VGG16在imageNet）\n嫁接到Pascal上，不用输出1000类，输出21类即可（加一个不在考虑的类别）\n拿出卷积部分，再对最后的全连接层进行稍微改造即可\n\n\n\n\n\n\n\n\n\n对一张图要提名的太多2000，然后对每一个提名做卷积提取特征，也就是2000次，那么运行效率会比较慢\n引出Fast R-CNN。\n\n\n\n### Fast R-CNN\n一个图片经过卷积后的特征映射，信息量降低了，但对物体的分类的重要信息都保留下来了，所以只对特征映射做区域提名，进一步的卷积提取特征，不在原图上做区域提名，因此减少了计算量。\n\nselected search是在cpu上运行的，因此想到对区域提名部分再做一个卷积操作，使整个操作都能在GPU上完成。\n### Faster RCNN\n两个分支：区域提名；特征提取以及方框位置的分类的计算。（框的位置，框内部是什么）\n一眼：看框打在哪儿。\n两眼：看框内是什么\n\n为了考虑实时性，达到在线预测的目的。\n\n### YOLO（You Only Look Once）\n整张图像输入，直接输出回归层 Bounding Box的位置和Bounding Box所属的类别\n\n将一幅图像分成SxS个网格(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object。\n\n缺点：当被检测目标集中在某个区域时效果很差。因为每个格子只能对应预测一个bounding box和类别概率，因此无法预测邻近区域的物体，检测精度下降了。\n\nSSD（Single Shot Multi-box Detector）\npredict object（物体），以及其 归属类别的 score（得分）；同时，在 feature map 上使用小的卷积核，去 predict 一系列 bounding boxes 的 box offsets。\n\n目标检测再进一步------行为动作捕捉。\n\n\n\n\n\n## 论文目录：\n\n[2013-----RCNN----Rich feature hierarchies for accurate object detection and semantic segmentation](https://zhuanlan.zhihu.com/p/29936564)\n\n2013----Deep Neural Networks for Object Detection\n\n2014----SPPnet----Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-sppnet\n\n[2015----Fast R-CNN](https://zhuanlan.zhihu.com/p/29953111)\n\n[2016----Faster R-CNN-Towards Real-Time Object Detection with Region Proposal Networks](https://zhuanlan.zhihu.com/p/29969145)\n\n2016----Inside-Outside Net_Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n\n[2016----R-FCN-Object Detection via Region-based Fully Convolutional Networks](https://zhuanlan.zhihu.com/p/30788068)\n\n[2016----SSD-Single Shot MultiBox Detector](https://zhuanlan.zhihu.com/p/30478644)\n\n[目标检测之SSD代码分析（MXNet版）](https://zhuanlan.zhihu.com/p/30553929)\n\n2016----YOLO9000-better,faster,stronger\n\n2016----You Only Look Once-Unified, Real-Time Object Detection\n\n2017----A-Fast-RCNN_Hard Positive Generation via Adversary for Object Detection\n\n[2017----Deformable Convolutional Networks](https://zhuanlan.zhihu.com/p/30927896)\n\n2017----DSOD_ Learning Deeply Supervised Object Detectors from Scratch\n\n[2017----Focal Loss for Dense Object Detection](https://zhuanlan.zhihu.com/p/30701067)\n\n2017----Mask R-CNN\n\n2017----Speed_Accuracy trade-offs for modern convolutional object detectors\n\n[2017----Light-Head R-CNN_In Defense of Two-Stage Object Detector](https://zhuanlan.zhihu.com/p/31389174)\n\n\n\n[^1]: Nulla facilisi. Ut feugiat.\n[^2]: Nullam dui erat, malesuada eget viverra non. [malesuada](https://none)\n[^3]: Phasellus turpis odio, scelerisque eu purus ac\n",[[1525258833464,["ww@DESKTOP-CF9KSFJ",[[1,414,"![v2-0659a27df35fd2f62cd00127ca8d1a21_b]($res/v2-0659a27df35fd2f62cd00127ca8d1a21_b.jpg)\n\n"]],[413,413],[503,503]]],[1525258848959,["ww@DESKTOP-CF9KSFJ",[[1,505,"\n"]],[503,503],[504,504]]],[1525258853258,["ww@DESKTOP-CF9KSFJ",[[1,504,"对于bounding box的定位精度，有一个很重要的概念： 因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。 它定义了两个bounding box的重叠度，如下图所示："]],[504,504],[610,610]]],[1525258853477,["ww@DESKTOP-CF9KSFJ",[[1,612,"\n"]],[610,610],[611,611]]],[1525258854061,["ww@DESKTOP-CF9KSFJ",[[1,613,"\n"]],[611,611],[612,612]]],[1525258865960,["ww@DESKTOP-CF9KSFJ",[[1,613,"![v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_b]($res/v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_b.jpg)\n\n"]],[612,612],[702,702]]]]],["82e412d3-b796-4f42-b0ad-bace0a249844",1525258959288,"---\nstyle: plain\n---\n\nObject detection\n===========\n\n[[toc]]\n\n## 初步介绍\n卷积（边缘检测等等）\n![20170325211712248]($res/20170325211712248.gif)\n\n* 特征提取\n![20180502180257]($res/20180502180257.png)\n\n**Pooling（压缩）**\n![20170325211641810]($res/20170325211641810.gif)\n\n**Padding**\n![20170325220130828]($res/20170325220130828.gif)\n\n**重叠度（IOU）:**\n物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。\n\n![v2-0659a27df35fd2f62cd00127ca8d1a21_b]($res/v2-0659a27df35fd2f62cd00127ca8d1a21_b.jpg)\n\n对于bounding box的定位精度，有一个很重要的概念： 因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。 它定义了两个bounding box的重叠度，如下图所示：\n\n\n![v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_b]($res/v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_b.jpg)\n\n\n上下采样\nVGG16  被很多目标检测用作特征提取\n* 16 weight layers\n* 13 convs + 3 fcs\n* 5 conv blocks\n\n\n```python\nINPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\n![20170325211641810]($res/20170325211641810.gif)\n\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n`````````\n![20180502180245]($res/20180502180245.png)\n\n\n## Object detection\n\ndataset: PASCAL VOC 、ImageNet\nPASCAL VOC 图片信息相对imageNet更加复杂\n\n### 思路：\n\n### 1.暴力解决：滑动窗口\n（已经猜到人脸大小设定窗口大小）\n![Lab-object-01]($res/Lab-object-01.png)\n穷举来实现，每个像素遍历\n\n### 2.Region Proposal（区域提名）\n\n\n选择性搜索（颜色、纹理、尺度、包含关系）\ncolor\ntexture\nsize\nfill\n进行合并\n![Lab-object-02]($res/Lab-object-02.png)\n光晕部分由色泽不同和图像之间的相似度，对其进行合并，最终分成几个大块儿，对应到下面就是这个蓝色的块儿，就是猜测到的有可能有目标物体的图块儿。然后对候选的几个框进行分类，这样极大提高了速度，从穷举变成有限次的分类问题。\n类似层次聚类，本身是无监督的\n\n\n一张图片 以提名的方式提出大概有目标物体的提名，然后用分类的方法去看看属于哪一类\nSSD Driven Hierarchical clustering\n\n### RCNN\n用卷积神经网络CNN，在这里就是用Region-based CNN （RCNN）\n![Lab-object-03]($res/Lab-object-03.png)\n\n创新点：\n* 采用CNN网络提取图像特征，从经验驱动的人造特征范式HOG、SIFT到数据驱动的表示学习范式，提高特征   对样本的表示能力；\n* 采用大样本下有监督预训练+小样本微调的方式解决小样本难以训练甚至过拟合等问题。\n\n**问题：**\n* 近10年以来，以人工经验特征为主导的物体检测任务mAP【物体类别和位置的平均精度】提升缓慢\n* 随着ReLu激励函数、dropout正则化手段和大规模图像样本集ILSVRC的出现，在2012年ImageNet大规模视觉识别挑战赛中，Hinton及他的学生采用CNN特征获得了最高的图像识别精确度；\n* 上述比赛后，引发了一股“是否可以采用CNN特征来提高当前一直停滞不前的物体检测准确率“的热潮。\n\n\n**流程:**\n1.输入图像\n2.每张图像生成1K~2K个候选区域\n3.对每个候选区域，使用深度网络提取特征（AlexNet、VGG、CNN......）\n4.1将特征送入每一类的SVM分类器，判别是否属于该类\n4.2使用回归器精细修正候选框位置\n\npretrained with RCNN（用VGG16在imageNet）\n嫁接到Pascal上，不用输出1000类，输出21类即可（加一个不在考虑的类别）\n拿出卷积部分，再对最后的全连接层进行稍微改造即可\n\n\n\n\n\n\n\n\n\n对一张图要提名的太多2000，然后对每一个提名做卷积提取特征，也就是2000次，那么运行效率会比较慢\n引出Fast R-CNN。\n\n\n\n### Fast R-CNN\n一个图片经过卷积后的特征映射，信息量降低了，但对物体的分类的重要信息都保留下来了，所以只对特征映射做区域提名，进一步的卷积提取特征，不在原图上做区域提名，因此减少了计算量。\n\nselected search是在cpu上运行的，因此想到对区域提名部分再做一个卷积操作，使整个操作都能在GPU上完成。\n### Faster RCNN\n两个分支：区域提名；特征提取以及方框位置的分类的计算。（框的位置，框内部是什么）\n一眼：看框打在哪儿。\n两眼：看框内是什么\n\n为了考虑实时性，达到在线预测的目的。\n\n### YOLO（You Only Look Once）\n整张图像输入，直接输出回归层 Bounding Box的位置和Bounding Box所属的类别\n\n将一幅图像分成SxS个网格(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object。\n\n缺点：当被检测目标集中在某个区域时效果很差。因为每个格子只能对应预测一个bounding box和类别概率，因此无法预测邻近区域的物体，检测精度下降了。\n\nSSD（Single Shot Multi-box Detector）\npredict object（物体），以及其 归属类别的 score（得分）；同时，在 feature map 上使用小的卷积核，去 predict 一系列 bounding boxes 的 box offsets。\n\n目标检测再进一步------行为动作捕捉。\n\n\n\n\n\n## 论文目录：\n\n[2013-----RCNN----Rich feature hierarchies for accurate object detection and semantic segmentation](https://zhuanlan.zhihu.com/p/29936564)\n\n2013----Deep Neural Networks for Object Detection\n\n2014----SPPnet----Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-sppnet\n\n[2015----Fast R-CNN](https://zhuanlan.zhihu.com/p/29953111)\n\n[2016----Faster R-CNN-Towards Real-Time Object Detection with Region Proposal Networks](https://zhuanlan.zhihu.com/p/29969145)\n\n2016----Inside-Outside Net_Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n\n[2016----R-FCN-Object Detection via Region-based Fully Convolutional Networks](https://zhuanlan.zhihu.com/p/30788068)\n\n[2016----SSD-Single Shot MultiBox Detector](https://zhuanlan.zhihu.com/p/30478644)\n\n[目标检测之SSD代码分析（MXNet版）](https://zhuanlan.zhihu.com/p/30553929)\n\n2016----YOLO9000-better,faster,stronger\n\n2016----You Only Look Once-Unified, Real-Time Object Detection\n\n2017----A-Fast-RCNN_Hard Positive Generation via Adversary for Object Detection\n\n[2017----Deformable Convolutional Networks](https://zhuanlan.zhihu.com/p/30927896)\n\n2017----DSOD_ Learning Deeply Supervised Object Detectors from Scratch\n\n[2017----Focal Loss for Dense Object Detection](https://zhuanlan.zhihu.com/p/30701067)\n\n2017----Mask R-CNN\n\n2017----Speed_Accuracy trade-offs for modern convolutional object detectors\n\n[2017----Light-Head R-CNN_In Defense of Two-Stage Object Detector](https://zhuanlan.zhihu.com/p/31389174)\n\n\n\n[^1]: Nulla facilisi. Ut feugiat.\n[^2]: Nullam dui erat, malesuada eget viverra non. [malesuada](https://none)\n[^3]: Phasellus turpis odio, scelerisque eu purus ac\n",[[1525258904557,["ww@DESKTOP-CF9KSFJ",[[1,703,"![v2-e26ffc0835bc30dede8d82989ef9e178_b]($res/v2-e26ffc0835bc30dede8d82989ef9e178_b.jpg)\n\n"]],[702,702],[792,792]]],[1525258933253,["ww@DESKTOP-CF9KSFJ",[[1,4503,"\n"]],[4501,4501],[4502,4502]]],[1525258933398,["ww@DESKTOP-CF9KSFJ",[[1,4504,"\n"]],[4502,4502],[4503,4503]]],[1525258933558,["ww@DESKTOP-CF9KSFJ",[[1,4505,"\n"]],[4503,4503],[4504,4504]]],[1525258933701,["ww@DESKTOP-CF9KSFJ",[[1,4506,"\n"]],[4504,4504],[4505,4505]]],[1525258933870,["ww@DESKTOP-CF9KSFJ",[[1,4507,"\n"]],[4505,4505],[4506,4506]]],[1525258936712,["ww@DESKTOP-CF9KSFJ",[[1,4503,"（**"]],[4503,4503],[4506,4506]]],[1525258937653,["ww@DESKTOP-CF9KSFJ",[[-1,4503,"（**"]],[4506,4506],[4503,4503]]],[1525258938760,["ww@DESKTOP-CF9KSFJ",[[1,4503,"** "]],[4503,4503],[4506,4506]]],[1525258941221,["ww@DESKTOP-CF9KSFJ",[[-1,4503,"** "]],[4506,4506],[4503,4503]]],[1525258941621,["ww@DESKTOP-CF9KSFJ",[[-1,4507,"\n"]],[4503,4503],[4502,4502]]],[1525258942006,["ww@DESKTOP-CF9KSFJ",[[-1,4506,"\n"]],[4502,4502],[4501,4501]]],[1525258974066,["ww@DESKTOP-CF9KSFJ",[[1,3769,"**候选框搜索阶段：**\n\n当我们输入一张图片时，我们要搜索出所有可能是物体的区域，这里采用的就是前面提到的Selective Search方法，通过这个算法我们搜索出2000个候选框。然后从上面的总流程图中可以看到，搜出的候选框是矩形的，而且是大小各不相同。然而CNN对输入图片的大小是有固定的，如果把搜索到的矩形选框不做处理，就扔进CNN中，肯定不行。因此对于每个输入的候选框都需要缩放到固定的大小。下面我们讲解要怎么进行缩放处理，为了简单起见我们假设下一阶段CNN所需要的输入图片大小是个正方形图片227*227。因为我们经过selective search 得到的是矩形框，paper试验了两种不同的处理方法：\n\n作者：晓雷\n链接：https://zhuanlan.zhihu.com/p/23006190\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"]],[3769,3769],[4167,4167]]],[1525258977989,["ww@DESKTOP-CF9KSFJ",[[-1,4080,"作者：晓雷\n链接：https://zhuanlan.zhihu.com/p/23006190\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"]],[4080,4167],[4080,4080]]],[1525258987926,["ww@DESKTOP-CF9KSFJ",[[1,4080,"(1)各向异性缩放"]],[4080,4080],[4089,4089]]],[1525258989501,["ww@DESKTOP-CF9KSFJ",[[1,4095,"\n"]],[4089,4089],[4090,4090]]],[1525259003685,["ww@DESKTOP-CF9KSFJ",[[1,4092,"(2)各向同性缩放"]],[4092,4092],[4101,4101]]],[1525259005205,["ww@DESKTOP-CF9KSFJ",[[1,4105,"\n"]],[4101,4101],[4102,4102]]]]],["f593fec2-59fc-42f0-8f67-1c78fe0327e2",1525259097886,"---\nstyle: plain\n---\n\nObject detection\n===========\n\n[[toc]]\n\n## 初步介绍\n卷积（边缘检测等等）\n![20170325211712248]($res/20170325211712248.gif)\n\n* 特征提取\n![20180502180257]($res/20180502180257.png)\n\n**Pooling（压缩）**\n![20170325211641810]($res/20170325211641810.gif)\n\n**Padding**\n![20170325220130828]($res/20170325220130828.gif)\n\n**重叠度（IOU）:**\n物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。\n\n![v2-0659a27df35fd2f62cd00127ca8d1a21_b]($res/v2-0659a27df35fd2f62cd00127ca8d1a21_b.jpg)\n\n对于bounding box的定位精度，有一个很重要的概念： 因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。 它定义了两个bounding box的重叠度，如下图所示：\n\n\n![v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_b]($res/v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_b.jpg)\n\n![v2-e26ffc0835bc30dede8d82989ef9e178_b]($res/v2-e26ffc0835bc30dede8d82989ef9e178_b.jpg)\n\n\n上下采样\nVGG16  被很多目标检测用作特征提取\n* 16 weight layers\n* 13 convs + 3 fcs\n* 5 conv blocks\n\n\n```python\nINPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\n![20170325211641810]($res/20170325211641810.gif)\n\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n`````````\n![20180502180245]($res/20180502180245.png)\n\n\n## Object detection\n\ndataset: PASCAL VOC 、ImageNet\nPASCAL VOC 图片信息相对imageNet更加复杂\n\n### 思路：\n\n### 1.暴力解决：滑动窗口\n（已经猜到人脸大小设定窗口大小）\n![Lab-object-01]($res/Lab-object-01.png)\n穷举来实现，每个像素遍历\n\n### 2.Region Proposal（区域提名）\n\n\n选择性搜索（颜色、纹理、尺度、包含关系）\ncolor\ntexture\nsize\nfill\n进行合并\n![Lab-object-02]($res/Lab-object-02.png)\n光晕部分由色泽不同和图像之间的相似度，对其进行合并，最终分成几个大块儿，对应到下面就是这个蓝色的块儿，就是猜测到的有可能有目标物体的图块儿。然后对候选的几个框进行分类，这样极大提高了速度，从穷举变成有限次的分类问题。\n类似层次聚类，本身是无监督的\n\n\n一张图片 以提名的方式提出大概有目标物体的提名，然后用分类的方法去看看属于哪一类\nSSD Driven Hierarchical clustering\n\n### RCNN\n用卷积神经网络CNN，在这里就是用Region-based CNN （RCNN）\n![Lab-object-03]($res/Lab-object-03.png)\n\n创新点：\n* 采用CNN网络提取图像特征，从经验驱动的人造特征范式HOG、SIFT到数据驱动的表示学习范式，提高特征   对样本的表示能力；\n* 采用大样本下有监督预训练+小样本微调的方式解决小样本难以训练甚至过拟合等问题。\n\n**问题：**\n* 近10年以来，以人工经验特征为主导的物体检测任务mAP【物体类别和位置的平均精度】提升缓慢\n* 随着ReLu激励函数、dropout正则化手段和大规模图像样本集ILSVRC的出现，在2012年ImageNet大规模视觉识别挑战赛中，Hinton及他的学生采用CNN特征获得了最高的图像识别精确度；\n* 上述比赛后，引发了一股“是否可以采用CNN特征来提高当前一直停滞不前的物体检测准确率“的热潮。\n\n\n**流程:**\n1.输入图像\n2.每张图像生成1K~2K个候选区域\n3.对每个候选区域，使用深度网络提取特征（AlexNet、VGG、CNN......）\n4.1将特征送入每一类的SVM分类器，判别是否属于该类\n4.2使用回归器精细修正候选框位置\n\npretrained with RCNN（用VGG16在imageNet）\n嫁接到Pascal上，不用输出1000类，输出21类即可（加一个不在考虑的类别）\n拿出卷积部分，再对最后的全连接层进行稍微改造即可\n\n\n\n**候选框搜索阶段：**\n\n当我们输入一张图片时，我们要搜索出所有可能是物体的区域，这里采用的就是前面提到的Selective Search方法，通过这个算法我们搜索出2000个候选框。然后从上面的总流程图中可以看到，搜出的候选框是矩形的，而且是大小各不相同。然而CNN对输入图片的大小是有固定的，如果把搜索到的矩形选框不做处理，就扔进CNN中，肯定不行。因此对于每个输入的候选框都需要缩放到固定的大小。下面我们讲解要怎么进行缩放处理，为了简单起见我们假设下一阶段CNN所需要的输入图片大小是个正方形图片227*227。因为我们经过selective search 得到的是矩形框，paper试验了两种不同的处理方法：\n\n(1)各向异性缩放\n\n\n(2)各向同性缩放\n\n\n\n\n对一张图要提名的太多2000，然后对每一个提名做卷积提取特征，也就是2000次，那么运行效率会比较慢\n引出Fast R-CNN。\n\n\n\n### Fast R-CNN\n一个图片经过卷积后的特征映射，信息量降低了，但对物体的分类的重要信息都保留下来了，所以只对特征映射做区域提名，进一步的卷积提取特征，不在原图上做区域提名，因此减少了计算量。\n\nselected search是在cpu上运行的，因此想到对区域提名部分再做一个卷积操作，使整个操作都能在GPU上完成。\n### Faster RCNN\n两个分支：区域提名；特征提取以及方框位置的分类的计算。（框的位置，框内部是什么）\n一眼：看框打在哪儿。\n两眼：看框内是什么\n\n为了考虑实时性，达到在线预测的目的。\n\n### YOLO（You Only Look Once）\n整张图像输入，直接输出回归层 Bounding Box的位置和Bounding Box所属的类别\n\n将一幅图像分成SxS个网格(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object。\n\n缺点：当被检测目标集中在某个区域时效果很差。因为每个格子只能对应预测一个bounding box和类别概率，因此无法预测邻近区域的物体，检测精度下降了。\n\nSSD（Single Shot Multi-box Detector）\npredict object（物体），以及其 归属类别的 score（得分）；同时，在 feature map 上使用小的卷积核，去 predict 一系列 bounding boxes 的 box offsets。\n\n目标检测再进一步------行为动作捕捉。\n\n\n\n\n\n\n\n\n## 论文目录：\n\n[2013-----RCNN----Rich feature hierarchies for accurate object detection and semantic segmentation](https://zhuanlan.zhihu.com/p/29936564)\n\n2013----Deep Neural Networks for Object Detection\n\n2014----SPPnet----Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-sppnet\n\n[2015----Fast R-CNN](https://zhuanlan.zhihu.com/p/29953111)\n\n[2016----Faster R-CNN-Towards Real-Time Object Detection with Region Proposal Networks](https://zhuanlan.zhihu.com/p/29969145)\n\n2016----Inside-Outside Net_Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n\n[2016----R-FCN-Object Detection via Region-based Fully Convolutional Networks](https://zhuanlan.zhihu.com/p/30788068)\n\n[2016----SSD-Single Shot MultiBox Detector](https://zhuanlan.zhihu.com/p/30478644)\n\n[目标检测之SSD代码分析（MXNet版）](https://zhuanlan.zhihu.com/p/30553929)\n\n2016----YOLO9000-better,faster,stronger\n\n2016----You Only Look Once-Unified, Real-Time Object Detection\n\n2017----A-Fast-RCNN_Hard Positive Generation via Adversary for Object Detection\n\n[2017----Deformable Convolutional Networks](https://zhuanlan.zhihu.com/p/30927896)\n\n2017----DSOD_ Learning Deeply Supervised Object Detectors from Scratch\n\n[2017----Focal Loss for Dense Object Detection](https://zhuanlan.zhihu.com/p/30701067)\n\n2017----Mask R-CNN\n\n2017----Speed_Accuracy trade-offs for modern convolutional object detectors\n\n[2017----Light-Head R-CNN_In Defense of Two-Stage Object Detector](https://zhuanlan.zhihu.com/p/31389174)\n\n\n\n[^1]: Nulla facilisi. Ut feugiat.\n[^2]: Nullam dui erat, malesuada eget viverra non. [malesuada](https://none)\n[^3]: Phasellus turpis odio, scelerisque eu purus ac\n",[[1525259044794,["ww@DESKTOP-CF9KSFJ",[[1,4354,"![v2-59449e8409b943f384c4cc3bf789d8b9_b]($res/v2-59449e8409b943f384c4cc3bf789d8b9_b.jpg)\n\n"]],[4103,4103],[4443,4443]]],[1525259049170,["ww@DESKTOP-CF9KSFJ",[[-1,4354,"![v2-59449e8409b943f384c4cc3bf789d8b9_b]($res/v2-59449e8409b943f384c4cc3bf789d8b9_b.jpg)"]],[4354,4442],[4354,4354]]],[1525259051772,["ww@DESKTOP-CF9KSFJ",[[1,4103,"![v2-59449e8409b943f384c4cc3bf789d8b9_b]($res/v2-59449e8409b943f384c4cc3bf789d8b9_b.jpg)"]],[4103,4103],[4191,4191]]],[1525259090717,["ww@DESKTOP-CF9KSFJ",[[1,4194,"\n"]],[4192,4192],[4193,4193]]],[1525259090869,["ww@DESKTOP-CF9KSFJ",[[1,4195,"\n"]],[4193,4193],[4194,4194]]],[1525259092559,["ww@DESKTOP-CF9KSFJ",[[1,4193,"A、先扩充后裁剪： 直接在原始图片中，把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充；如上图(B)所示;\n\nB、先裁剪后扩充：先把bounding box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用bounding box的像素颜色均值),如上图(C)所示;\n\n对于上面的异性、同性缩放，文献还有个padding处理，上面的示意图中第1、3行就是结合了padding=0,第2、4行结果图采用padding=16的结果。经过最后的试验，作者发现采用各向异性缩放、padding=16的精度最高。\n\n作者：晓雷\n链接：https://zhuanlan.zhihu.com/p/23006190\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"]],[4193,4193],[4594,4594]]],[1525259098791,["ww@DESKTOP-CF9KSFJ",[[-1,4507,"作者：晓雷\n链接：https://zhuanlan.zhihu.com/p/23006190\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"]],[4507,4594],[4507,4507]]],[1525259099189,["ww@DESKTOP-CF9KSFJ",[[-1,4509,"\n"]],[4507,4507],[4506,4506]]],[1525259103197,["ww@DESKTOP-CF9KSFJ",[[-1,4091,"\n"]],[4091,4091],[4090,4090]]],[1525259103949,["ww@DESKTOP-CF9KSFJ",[[-1,4090,"\n"]],[4090,4090],[4089,4089]]],[1525259106342,["ww@DESKTOP-CF9KSFJ",[[-1,4100,"\n"]],[4100,4100],[4099,4099]]],[1525259112389,["ww@DESKTOP-CF9KSFJ",[[1,4506,"\n"]],[4502,4502],[4503,4503]]],[1525259112533,["ww@DESKTOP-CF9KSFJ",[[1,4507,"\n"]],[4503,4503],[4504,4504]]],[1525259177736,["ww@DESKTOP-CF9KSFJ",[[1,4504,"a、网络结构设计阶段\n\n网络架构两个可选方案：第一选择经典的Alexnet；第二选择VGG16。经过测试Alexnet精度为58.5%，VGG16精度为66%。VGG这个模型的特点是选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是Alexnet的7倍。后面为了简单起见，我们就直接选用Alexnet，并进行讲解；Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、 f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。\n\n作者：晓雷\n链接：https://zhuanlan.zhihu.com/p/23006190\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"]],[4504,4504],[4873,4873]]],[1525259184038,["ww@DESKTOP-CF9KSFJ",[[1,4504,"### "]],[4504,4504],[4508,4508]]],[1525259187829,["ww@DESKTOP-CF9KSFJ",[[-1,4790,"作者：晓雷\n链接：https://zhuanlan.zhihu.com/p/23006190\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"]],[4790,4877],[4790,4790]]],[1525259214045,["ww@DESKTOP-CF9KSFJ",[[1,4790,"### b、网络有监督预训练阶段 （图片数据库：ImageNet ILSVC ）"]],[4790,4790],[4830,4830]]],[1525259215852,["ww@DESKTOP-CF9KSFJ",[[1,4834,"\n"]],[4830,4830],[4831,4831]]],[1525259216629,["ww@DESKTOP-CF9KSFJ",[[1,4835,"\n"]],[4831,4831],[4832,4832]]],[1525259233310,["ww@DESKTOP-CF9KSFJ",[[1,4832,"fine0"]],[4832,4832],[4837,4837]]],[1525259234028,["ww@DESKTOP-CF9KSFJ",[[-1,4836,"0"]],[4837,4837],[4836,4836]]],[1525259238686,["ww@DESKTOP-CF9KSFJ",[[1,4836,"-tune（）"]],[4836,4836],[4843,4843]]],[1525259244854,["ww@DESKTOP-CF9KSFJ",[[1,4842,"采用AlexNet"]],[4842,4842],[4851,4851]]],[1525259265238,["ww@DESKTOP-CF9KSFJ",[[1,4852,"优化采用随机梯度下降法，学习率：0，"]],[4852,4852],[4870,4870]]],[1525259266441,["ww@DESKTOP-CF9KSFJ",[[-1,4869,"，"]],[4870,4870],[4869,4869]]],[1525259266735,["ww@DESKTOP-CF9KSFJ",[[1,4869,",001"]],[4869,4869],[4873,4873]]],[1525259269500,["ww@DESKTOP-CF9KSFJ",[[-1,4869,","]],[4870,4870],[4869,4869]]],[1525259270030,["ww@DESKTOP-CF9KSFJ",[[1,4869,"。"]],[4869,4869],[4870,4870]]],[1525259270628,["ww@DESKTOP-CF9KSFJ",[[-1,4869,"。"]],[4870,4870],[4869,4869]]],[1525259271933,["ww@DESKTOP-CF9KSFJ",[[1,4869,"."]],[4869,4869],[4870,4870]]],[1525259274990,["ww@DESKTOP-CF9KSFJ",[[1,4877,"\n"]],[4874,4874],[4875,4875]]],[1525259275668,["ww@DESKTOP-CF9KSFJ",[[1,4878,"\n"]],[4875,4875],[4876,4876]]],[1525259298118,["ww@DESKTOP-CF9KSFJ",[[1,4875,"### "]],[4875,4875],[4879,4879]]],[1525259306660,["ww@DESKTOP-CF9KSFJ",[[-1,4875,"### "]],[4879,4879],[4875,4875]]],[1525259307380,["ww@DESKTOP-CF9KSFJ",[[1,4879,"\n"]],[4875,4875],[4876,4876]]],[1525259307532,["ww@DESKTOP-CF9KSFJ",[[1,4880,"\n"]],[4876,4876],[4877,4877]]],[1525259308606,["ww@DESKTOP-CF9KSFJ",[[1,4877,"xuanq"]],[4877,4877],[4882,4882]]],[1525259309588,["ww@DESKTOP-CF9KSFJ",[[-1,4877,"xuanq"]],[4882,4882],[4877,4877]]],[1525259316734,["ww@DESKTOP-CF9KSFJ",[[1,4877,"选取IOU大于0"]],[4877,4877],[4885,4885]]],[1525259317964,["ww@DESKTOP-CF9KSFJ",[[-1,4884,"0"]],[4885,4885],[4884,4884]]],[1525259319953,["ww@DESKTOP-CF9KSFJ",[[1,4884,"0.5"]],[4884,4884],[4887,4887]]],[1525259331772,["ww@DESKTOP-CF9KSFJ",[[-1,4882,"大于"]],[4884,4884],[4882,4882]]],[1525259333310,["ww@DESKTOP-CF9KSFJ",[[1,4882,"》"]],[4882,4882],[4883,4883]]],[1525259334245,["ww@DESKTOP-CF9KSFJ",[[-1,4882,"》"]],[4883,4883],[4882,4882]]],[1525259335357,["ww@DESKTOP-CF9KSFJ",[[1,4882,">"]],[4882,4882],[4883,4883]]],[1525259337733,["ww@DESKTOP-CF9KSFJ",[[1,4886," zhe"]],[4886,4886],[4890,4890]]],[1525259338756,["ww@DESKTOP-CF9KSFJ",[[-1,4887,"zhen"]],[4891,4891],[4887,4887]]],[1525259350710,["ww@DESKTOP-CF9KSFJ",[[1,4887,"+ or - yan"]],[4887,4887],[4897,4897]]],[1525259351364,["ww@DESKTOP-CF9KSFJ",[[-1,4894,"yan"]],[4897,4897],[4894,4894]]],[1525259353010,["ww@DESKTOP-CF9KSFJ",[[1,4894,"样本"]],[4894,4894],[4896,4896]]],[1525259354268,["ww@DESKTOP-CF9KSFJ",[[1,4900,"\n"]],[4896,4896],[4897,4897]]],[1525259354590,["ww@DESKTOP-CF9KSFJ",[[1,4901,"\n"]],[4897,4897],[4898,4898]]],[1525259376448,["ww@DESKTOP-CF9KSFJ",[[1,4899,"### "]],[4899,4899],[4903,4903]]],[1525259377076,["ww@DESKTOP-CF9KSFJ",[[1,4906,"\n"]],[4903,4903],[4904,4904]]],[1525259377972,["ww@DESKTOP-CF9KSFJ",[[1,4904,"**SVM训练、测试阶段**\n\n训练阶段：\n\n这是一个二分类问题，我么假设我们要检测车辆。我们知道只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。但问题是当我们的检测窗口只有部分包含物体，那该怎么定义正负样本呢？作者测试了IOU阈值各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后通过训练发现，如果选择IOU阈值为0.3效果最好（选择为0精度下降了4个百分点，选择0.5精度下降了5个百分点）,即当重叠度小于0.3的时候，我们就把它标注为负样本。一旦CNN f7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到2000*4096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096*N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)，就可以得到结果了。\n\n作者：晓雷\n链接：https://zhuanlan.zhihu.com/p/23006190\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"]],[4904,4904],[5429,5429]]],[1525259383260,["ww@DESKTOP-CF9KSFJ",[[-1,5342,"作者：晓雷\n链接：https://zhuanlan.zhihu.com/p/23006190\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"]],[5342,5429],[5342,5342]]],[1525259383900,["ww@DESKTOP-CF9KSFJ",[[-1,5344,"\n"]],[5342,5342],[5341,5341]]],[1525259388532,["ww@DESKTOP-CF9KSFJ",[[-1,4899,"###"]],[4902,4902],[4899,4899]]],[1525259391091,["ww@DESKTOP-CF9KSFJ",[[-1,4901,"**"]],[4903,4903],[4901,4901]]],[1525259394701,["ww@DESKTOP-CF9KSFJ",[[1,4901,"c、"]],[4901,4901],[4903,4903]]],[1525259457881,[null,[[-1,4900,"对"],[1,4901,"\n"],[1,5341,"对"]],[4900,4900],[5342,5342]]],[1525259457882,[null,[[1,4900,"对"],[-1,4900,"\n"],[-1,5341,"对"]],[5342,5342],[4900,4900]]],[1525259399461,["ww@DESKTOP-CF9KSFJ",[[1,4901,"### "]],[4901,4901],[4905,4905]]],[1525259402693,["ww@DESKTOP-CF9KSFJ",[[-1,4917,"**"]],[4919,4919],[4917,4917]]],[1525259424180,["ww@DESKTOP-CF9KSFJ",[[1,5055,"\n"]],[5055,5055],[5056,5056]]],[1525259453715,["ww@DESKTOP-CF9KSFJ",[[1,5344,"\n"]],[5342,5342],[5343,5343]]],[1525259453828,["ww@DESKTOP-CF9KSFJ",[[1,5345,"\n"]],[5343,5343],[5344,5344]]]]],["e4aef571-a29a-45f2-b346-68766bb463f6",1525259527634,"---\nstyle: plain\n---\n\nObject detection\n===========\n\n[[toc]]\n\n## 初步介绍\n卷积（边缘检测等等）\n![20170325211712248]($res/20170325211712248.gif)\n\n* 特征提取\n![20180502180257]($res/20180502180257.png)\n\n**Pooling（压缩）**\n![20170325211641810]($res/20170325211641810.gif)\n\n**Padding**\n![20170325220130828]($res/20170325220130828.gif)\n\n**重叠度（IOU）:**\n物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。\n\n![v2-0659a27df35fd2f62cd00127ca8d1a21_b]($res/v2-0659a27df35fd2f62cd00127ca8d1a21_b.jpg)\n\n对于bounding box的定位精度，有一个很重要的概念： 因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。 它定义了两个bounding box的重叠度，如下图所示：\n\n\n![v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_b]($res/v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_b.jpg)\n\n![v2-e26ffc0835bc30dede8d82989ef9e178_b]($res/v2-e26ffc0835bc30dede8d82989ef9e178_b.jpg)\n\n\n上下采样\nVGG16  被很多目标检测用作特征提取\n* 16 weight layers\n* 13 convs + 3 fcs\n* 5 conv blocks\n\n\n```python\nINPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\n![20170325211641810]($res/20170325211641810.gif)\n\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n`````````\n![20180502180245]($res/20180502180245.png)\n\n\n## Object detection\n\ndataset: PASCAL VOC 、ImageNet\nPASCAL VOC 图片信息相对imageNet更加复杂\n\n### 思路：\n\n### 1.暴力解决：滑动窗口\n（已经猜到人脸大小设定窗口大小）\n![Lab-object-01]($res/Lab-object-01.png)\n穷举来实现，每个像素遍历\n\n### 2.Region Proposal（区域提名）\n\n\n选择性搜索（颜色、纹理、尺度、包含关系）\ncolor\ntexture\nsize\nfill\n进行合并\n![Lab-object-02]($res/Lab-object-02.png)\n光晕部分由色泽不同和图像之间的相似度，对其进行合并，最终分成几个大块儿，对应到下面就是这个蓝色的块儿，就是猜测到的有可能有目标物体的图块儿。然后对候选的几个框进行分类，这样极大提高了速度，从穷举变成有限次的分类问题。\n类似层次聚类，本身是无监督的\n\n\n一张图片 以提名的方式提出大概有目标物体的提名，然后用分类的方法去看看属于哪一类\nSSD Driven Hierarchical clustering\n\n### RCNN\n用卷积神经网络CNN，在这里就是用Region-based CNN （RCNN）\n![Lab-object-03]($res/Lab-object-03.png)\n\n创新点：\n* 采用CNN网络提取图像特征，从经验驱动的人造特征范式HOG、SIFT到数据驱动的表示学习范式，提高特征   对样本的表示能力；\n* 采用大样本下有监督预训练+小样本微调的方式解决小样本难以训练甚至过拟合等问题。\n\n**问题：**\n* 近10年以来，以人工经验特征为主导的物体检测任务mAP【物体类别和位置的平均精度】提升缓慢\n* 随着ReLu激励函数、dropout正则化手段和大规模图像样本集ILSVRC的出现，在2012年ImageNet大规模视觉识别挑战赛中，Hinton及他的学生采用CNN特征获得了最高的图像识别精确度；\n* 上述比赛后，引发了一股“是否可以采用CNN特征来提高当前一直停滞不前的物体检测准确率“的热潮。\n\n\n**流程:**\n1.输入图像\n2.每张图像生成1K~2K个候选区域\n3.对每个候选区域，使用深度网络提取特征（AlexNet、VGG、CNN......）\n4.1将特征送入每一类的SVM分类器，判别是否属于该类\n4.2使用回归器精细修正候选框位置\n\npretrained with RCNN（用VGG16在imageNet）\n嫁接到Pascal上，不用输出1000类，输出21类即可（加一个不在考虑的类别）\n拿出卷积部分，再对最后的全连接层进行稍微改造即可\n\n\n\n**候选框搜索阶段：**\n\n当我们输入一张图片时，我们要搜索出所有可能是物体的区域，这里采用的就是前面提到的Selective Search方法，通过这个算法我们搜索出2000个候选框。然后从上面的总流程图中可以看到，搜出的候选框是矩形的，而且是大小各不相同。然而CNN对输入图片的大小是有固定的，如果把搜索到的矩形选框不做处理，就扔进CNN中，肯定不行。因此对于每个输入的候选框都需要缩放到固定的大小。下面我们讲解要怎么进行缩放处理，为了简单起见我们假设下一阶段CNN所需要的输入图片大小是个正方形图片227*227。因为我们经过selective search 得到的是矩形框，paper试验了两种不同的处理方法：\n\n(1)各向异性缩放\n(2)各向同性缩放\n![v2-59449e8409b943f384c4cc3bf789d8b9_b]($res/v2-59449e8409b943f384c4cc3bf789d8b9_b.jpg)\n\nA、先扩充后裁剪： 直接在原始图片中，把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充；如上图(B)所示;\n\nB、先裁剪后扩充：先把bounding box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用bounding box的像素颜色均值),如上图(C)所示;\n\n对于上面的异性、同性缩放，文献还有个padding处理，上面的示意图中第1、3行就是结合了padding=0,第2、4行结果图采用padding=16的结果。经过最后的试验，作者发现采用各向异性缩放、padding=16的精度最高。\n\n### a、网络结构设计阶段\n\n网络架构两个可选方案：第一选择经典的Alexnet；第二选择VGG16。经过测试Alexnet精度为58.5%，VGG16精度为66%。VGG这个模型的特点是选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是Alexnet的7倍。后面为了简单起见，我们就直接选用Alexnet，并进行讲解；Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、 f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。\n\n### b、网络有监督预训练阶段 （图片数据库：ImageNet ILSVC ）\n\nfine-tune（采用AlexNet）优化采用随机梯度下降法，学习率：0.001\n\n\n\n选取IOU>0.5 + or - 样本\n\n\n \n### c、SVM训练、测试阶段\n\n训练阶段：\n\n这是一个二分类问题，我么假设我们要检测车辆。我们知道只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。但问题是当我们的检测窗口只有部分包含物体，那该怎么定义正负样本呢？\n作者测试了IOU阈值各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后通过训练发现，如果选择IOU阈值为0.3效果最好（选择为0精度下降了4个百分点，选择0.5精度下降了5个百分点）,即当重叠度小于0.3的时候，我们就把它标注为负样本。一旦CNN f7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到2000*4096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096*N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)，就可以得到结果了。\n\n\n\n\n\n对一张图要提名的太多2000，然后对每一个提名做卷积提取特征，也就是2000次，那么运行效率会比较慢\n引出Fast R-CNN。\n\n\n\n### Fast R-CNN\n一个图片经过卷积后的特征映射，信息量降低了，但对物体的分类的重要信息都保留下来了，所以只对特征映射做区域提名，进一步的卷积提取特征，不在原图上做区域提名，因此减少了计算量。\n\nselected search是在cpu上运行的，因此想到对区域提名部分再做一个卷积操作，使整个操作都能在GPU上完成。\n### Faster RCNN\n\n\n两个分支：区域提名；特征提取以及方框位置的分类的计算。（框的位置，框内部是什么）\n一眼：看框打在哪儿。\n两眼：看框内是什么\n\n为了考虑实时性，达到在线预测的目的。\n\n### YOLO（You Only Look Once）\n整张图像输入，直接输出回归层 Bounding Box的位置和Bounding Box所属的类别\n\n将一幅图像分成SxS个网格(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object。\n\n缺点：当被检测目标集中在某个区域时效果很差。因为每个格子只能对应预测一个bounding box和类别概率，因此无法预测邻近区域的物体，检测精度下降了。\n\nSSD（Single Shot Multi-box Detector）\npredict object（物体），以及其 归属类别的 score（得分）；同时，在 feature map 上使用小的卷积核，去 predict 一系列 bounding boxes 的 box offsets。\n\n目标检测再进一步------行为动作捕捉。\n\n\n\n\n\n\n\n\n## 论文目录：\n\n[2013-----RCNN----Rich feature hierarchies for accurate object detection and semantic segmentation](https://zhuanlan.zhihu.com/p/29936564)\n\n2013----Deep Neural Networks for Object Detection\n\n2014----SPPnet----Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-sppnet\n\n[2015----Fast R-CNN](https://zhuanlan.zhihu.com/p/29953111)\n\n[2016----Faster R-CNN-Towards Real-Time Object Detection with Region Proposal Networks](https://zhuanlan.zhihu.com/p/29969145)\n\n2016----Inside-Outside Net_Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n\n[2016----R-FCN-Object Detection via Region-based Fully Convolutional Networks](https://zhuanlan.zhihu.com/p/30788068)\n\n[2016----SSD-Single Shot MultiBox Detector](https://zhuanlan.zhihu.com/p/30478644)\n\n[目标检测之SSD代码分析（MXNet版）](https://zhuanlan.zhihu.com/p/30553929)\n\n2016----YOLO9000-better,faster,stronger\n\n2016----You Only Look Once-Unified, Real-Time Object Detection\n\n2017----A-Fast-RCNN_Hard Positive Generation via Adversary for Object Detection\n\n[2017----Deformable Convolutional Networks](https://zhuanlan.zhihu.com/p/30927896)\n\n2017----DSOD_ Learning Deeply Supervised Object Detectors from Scratch\n\n[2017----Focal Loss for Dense Object Detection](https://zhuanlan.zhihu.com/p/30701067)\n\n2017----Mask R-CNN\n\n2017----Speed_Accuracy trade-offs for modern convolutional object detectors\n\n[2017----Light-Head R-CNN_In Defense of Two-Stage Object Detector](https://zhuanlan.zhihu.com/p/31389174)\n\n\n\n[^1]: Nulla facilisi. Ut feugiat.\n[^2]: Nullam dui erat, malesuada eget viverra non. [malesuada](https://none)\n[^3]: Phasellus turpis odio, scelerisque eu purus ac\n",[[1525259473990,["ww@DESKTOP-CF9KSFJ",[[1,5344,"![v2-3ef21dd028fd210f92107c1ded528045_b]($res/v2-3ef21dd028fd210f92107c1ded528045_b.jpg)\n\n"]],[5343,5343],[5433,5433]]],[1525259523076,["ww@DESKTOP-CF9KSFJ",[[1,4877,"CNN阶段：打标签，"]],[4877,4877],[4887,4887]]],[1525259553839,["ww@DESKTOP-CF9KSFJ",[[1,4887,"selective search VS 人工"]],[4887,4887],[4909,4909]]],[1525259556188,["ww@DESKTOP-CF9KSFJ",[[1,4909,"\n"]],[4909,4909],[4910,4910]]],[1525259558869,["ww@DESKTOP-CF9KSFJ",[[1,4910,"\t\t\t\t"]],[4910,4910],[4914,4914]]],[1525259580516,["ww@DESKTOP-CF9KSFJ",[[1,5473,"\n"]],[5470,5470],[5471,5471]]],[1525259580659,["ww@DESKTOP-CF9KSFJ",[[1,5474,"\n"]],[5471,5471],[5472,5472]]],[1525259581596,["ww@DESKTOP-CF9KSFJ",[[1,5472,"位置精修： 目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。 回归器：对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000。 输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。 训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框。\n\n作者：晓雷\n链接：https://zhuanlan.zhihu.com/p/23006190\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"]],[5472,5472],[5735,5735]]],[1525259585396,["ww@DESKTOP-CF9KSFJ",[[-1,5648,"作者：晓雷\n链接：https://zhuanlan.zhihu.com/p/23006190\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"]],[5648,5735],[5648,5648]]],[1525259585763,["ww@DESKTOP-CF9KSFJ",[[-1,5650,"\n"]],[5648,5648],[5647,5647]]],[1525259594693,["ww@DESKTOP-CF9KSFJ",[[1,5478,"\n"]],[5478,5478],[5479,5479]]],[1525259598078,["ww@DESKTOP-CF9KSFJ",[[1,5472,"**"]],[5472,5472],[5474,5474]]],[1525259602220,["ww@DESKTOP-CF9KSFJ",[[1,5479,"**"]],[5479,5479],[5481,5481]]],[1525259605727,["ww@DESKTOP-CF9KSFJ",[[1,4956,"**"]],[4956,4956],[4958,4958]]],[1525259608454,["ww@DESKTOP-CF9KSFJ",[[1,4963,"**"]],[4963,4963],[4965,4965]]],[1525259616795,["ww@DESKTOP-CF9KSFJ",[[1,5659,"\n"]],[5659,5659],[5660,5660]]],[1525259619844,["ww@DESKTOP-CF9KSFJ",[[1,5892,"\n"]],[5891,5891],[5892,5892]]],[1525259634531,["ww@DESKTOP-CF9KSFJ",[[-1,6295,"，"]],[6296,6296],[6295,6295]]],[1525259673259,["ww@DESKTOP-CF9KSFJ",[[1,5623,"\n"]],[5623,5623],[5624,5624]]],[1525259679340,["ww@DESKTOP-CF9KSFJ",[[1,5548,"\n"]],[5548,5548],[5549,5549]]],[1525259737075,["ww@DESKTOP-CF9KSFJ",[[-1,4504,"### "]],[4504,4508],[4504,4504]]],[1525259738621,["ww@DESKTOP-CF9KSFJ",[[1,4504,"** "]],[4504,4504],[4507,4507]]],[1525259740515,["ww@DESKTOP-CF9KSFJ",[[-1,4506," "]],[4507,4507],[4506,4506]]],[1525259743533,["ww@DESKTOP-CF9KSFJ",[[1,4516,"**"]],[4516,4516],[4518,4518]]],[1525259748956,["ww@DESKTOP-CF9KSFJ",[[-1,4790,"### "],[1,4794,"**"]],[4790,4794],[4792,4792]]],[1525259751925,["ww@DESKTOP-CF9KSFJ",[[1,4828,"**"]],[4828,4828],[4830,4830]]],[1525259755892,["ww@DESKTOP-CF9KSFJ",[[-1,4938,"### "],[1,4942,"**"]],[4938,4942],[4940,4940]]],[1525259758485,["ww@DESKTOP-CF9KSFJ",[[1,4952,"**"]],[4952,4952],[4954,4954]]],[1525259954395,["ww@DESKTOP-CF9KSFJ",[[1,793,"** **非极大值抑制（**NMS**）：**\n\nRCNN会从一张图片中找出n个可能是物体的矩形框，然后为每个矩形框为做类别分类概率："]],[793,793],[860,860]]],[1525259957233,["ww@DESKTOP-CF9KSFJ",[[-1,796,"**"]],[798,798],[796,796]]],[1525259966889,["ww@DESKTOP-CF9KSFJ",[[-1,808,"**"]],[810,810],[808,808]]],[1525259969138,["ww@DESKTOP-CF9KSFJ",[[-1,803,"**"]],[805,805],[803,803]]],[1525259971131,["ww@DESKTOP-CF9KSFJ",[[1,812,"\n"]],[810,810],[811,811]]],[1525259971330,["ww@DESKTOP-CF9KSFJ",[[1,813,"\n"]],[811,811],[812,812]]]]],["102c2e9b-f135-4e0b-8bf3-0b8cf9b3dd65",1525260043954,"---\nstyle: plain\n---\n\nObject detection\n===========\n\n[[toc]]\n\n## 初步介绍\n卷积（边缘检测等等）\n![20170325211712248]($res/20170325211712248.gif)\n\n* 特征提取\n![20180502180257]($res/20180502180257.png)\n\n**Pooling（压缩）**\n![20170325211641810]($res/20170325211641810.gif)\n\n**Padding**\n![20170325220130828]($res/20170325220130828.gif)\n\n**重叠度（IOU）:**\n物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。\n\n![v2-0659a27df35fd2f62cd00127ca8d1a21_b]($res/v2-0659a27df35fd2f62cd00127ca8d1a21_b.jpg)\n\n对于bounding box的定位精度，有一个很重要的概念： 因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。 它定义了两个bounding box的重叠度，如下图所示：\n\n\n![v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_b]($res/v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_b.jpg)\n\n![v2-e26ffc0835bc30dede8d82989ef9e178_b]($res/v2-e26ffc0835bc30dede8d82989ef9e178_b.jpg)\n\n** 非极大值抑制（NMS）：**\n\n\n\nRCNN会从一张图片中找出n个可能是物体的矩形框，然后为每个矩形框为做类别分类概率：\n上下采样\nVGG16  被很多目标检测用作特征提取\n* 16 weight layers\n* 13 convs + 3 fcs\n* 5 conv blocks\n\n\n```python\nINPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728\nCONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864\nPOOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728\nCONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456\nPOOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912\n![20170325211641810]($res/20170325211641810.gif)\n\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nCONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824\nPOOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nCONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296\nPOOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0\nFC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448\nFC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216\nFC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000\n`````````\n![20180502180245]($res/20180502180245.png)\n\n\n## Object detection\n\ndataset: PASCAL VOC 、ImageNet\nPASCAL VOC 图片信息相对imageNet更加复杂\n\n### 思路：\n\n### 1.暴力解决：滑动窗口\n（已经猜到人脸大小设定窗口大小）\n![Lab-object-01]($res/Lab-object-01.png)\n穷举来实现，每个像素遍历\n\n### 2.Region Proposal（区域提名）\n\n\n选择性搜索（颜色、纹理、尺度、包含关系）\ncolor\ntexture\nsize\nfill\n进行合并\n![Lab-object-02]($res/Lab-object-02.png)\n光晕部分由色泽不同和图像之间的相似度，对其进行合并，最终分成几个大块儿，对应到下面就是这个蓝色的块儿，就是猜测到的有可能有目标物体的图块儿。然后对候选的几个框进行分类，这样极大提高了速度，从穷举变成有限次的分类问题。\n类似层次聚类，本身是无监督的\n\n\n一张图片 以提名的方式提出大概有目标物体的提名，然后用分类的方法去看看属于哪一类\nSSD Driven Hierarchical clustering\n\n### RCNN\n用卷积神经网络CNN，在这里就是用Region-based CNN （RCNN）\n![Lab-object-03]($res/Lab-object-03.png)\n\n创新点：\n* 采用CNN网络提取图像特征，从经验驱动的人造特征范式HOG、SIFT到数据驱动的表示学习范式，提高特征   对样本的表示能力；\n* 采用大样本下有监督预训练+小样本微调的方式解决小样本难以训练甚至过拟合等问题。\n\n**问题：**\n* 近10年以来，以人工经验特征为主导的物体检测任务mAP【物体类别和位置的平均精度】提升缓慢\n* 随着ReLu激励函数、dropout正则化手段和大规模图像样本集ILSVRC的出现，在2012年ImageNet大规模视觉识别挑战赛中，Hinton及他的学生采用CNN特征获得了最高的图像识别精确度；\n* 上述比赛后，引发了一股“是否可以采用CNN特征来提高当前一直停滞不前的物体检测准确率“的热潮。\n\n\n**流程:**\n1.输入图像\n2.每张图像生成1K~2K个候选区域\n3.对每个候选区域，使用深度网络提取特征（AlexNet、VGG、CNN......）\n4.1将特征送入每一类的SVM分类器，判别是否属于该类\n4.2使用回归器精细修正候选框位置\n\npretrained with RCNN（用VGG16在imageNet）\n嫁接到Pascal上，不用输出1000类，输出21类即可（加一个不在考虑的类别）\n拿出卷积部分，再对最后的全连接层进行稍微改造即可\n\n\n\n**候选框搜索阶段：**\n\n当我们输入一张图片时，我们要搜索出所有可能是物体的区域，这里采用的就是前面提到的Selective Search方法，通过这个算法我们搜索出2000个候选框。然后从上面的总流程图中可以看到，搜出的候选框是矩形的，而且是大小各不相同。然而CNN对输入图片的大小是有固定的，如果把搜索到的矩形选框不做处理，就扔进CNN中，肯定不行。因此对于每个输入的候选框都需要缩放到固定的大小。下面我们讲解要怎么进行缩放处理，为了简单起见我们假设下一阶段CNN所需要的输入图片大小是个正方形图片227*227。因为我们经过selective search 得到的是矩形框，paper试验了两种不同的处理方法：\n\n(1)各向异性缩放\n(2)各向同性缩放\n![v2-59449e8409b943f384c4cc3bf789d8b9_b]($res/v2-59449e8409b943f384c4cc3bf789d8b9_b.jpg)\n\nA、先扩充后裁剪： 直接在原始图片中，把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充；如上图(B)所示;\n\nB、先裁剪后扩充：先把bounding box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用bounding box的像素颜色均值),如上图(C)所示;\n\n对于上面的异性、同性缩放，文献还有个padding处理，上面的示意图中第1、3行就是结合了padding=0,第2、4行结果图采用padding=16的结果。经过最后的试验，作者发现采用各向异性缩放、padding=16的精度最高。\n\n**a、网络结构设计阶段**\n\n网络架构两个可选方案：第一选择经典的Alexnet；第二选择VGG16。经过测试Alexnet精度为58.5%，VGG16精度为66%。VGG这个模型的特点是选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是Alexnet的7倍。后面为了简单起见，我们就直接选用Alexnet，并进行讲解；Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、 f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。\n\n**b、网络有监督预训练阶段 （图片数据库：ImageNet ILSVC ）**\n\nfine-tune（采用AlexNet）优化采用随机梯度下降法，学习率：0.001\n\n\n\nCNN阶段：打标签，selective search VS 人工\n\t\t\t\t选取IOU>0.5 + or - 样本\n\n\n \n**c、SVM训练、测试阶段**\n\n**训练阶段：**\n\n这是一个二分类问题，我么假设我们要检测车辆。我们知道只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。但问题是当我们的检测窗口只有部分包含物体，那该怎么定义正负样本呢？\n作者测试了IOU阈值各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后通过训练发现，如果选择IOU阈值为0.3效果最好（选择为0精度下降了4个百分点，选择0.5精度下降了5个百分点）,即当重叠度小于0.3的时候，我们就把它标注为负样本。一旦CNN f7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到2000*4096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096*N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)，就可以得到结果了。\n\n\n\n![v2-3ef21dd028fd210f92107c1ded528045_b]($res/v2-3ef21dd028fd210f92107c1ded528045_b.jpg)\n\n\n**位置精修：** \n目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。 \n回归器：对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000。 输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。 \n训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框。\n\n\n\n\n对一张图要提名的太多2000，然后对每一个提名做卷积提取特征，也就是2000次，那么运行效率会比较慢\n引出Fast R-CNN。\n\n\n\n### Fast R-CNN\n一个图片经过卷积后的特征映射，信息量降低了，但对物体的分类的重要信息都保留下来了，所以只对特征映射做区域提名，进一步的卷积提取特征，不在原图上做区域提名，因此减少了计算量。\n\nselected search是在cpu上运行的，因此想到对区域提名部分再做一个卷积操作，使整个操作都能在GPU上完成。\n\n### Faster RCNN\n\n\n两个分支：区域提名；特征提取以及方框位置的分类的计算。（框的位置，框内部是什么）\n一眼：看框打在哪儿。\n两眼：看框内是什么\n\n为了考虑实时性，达到在线预测的目的。\n\n### YOLO（You Only Look Once）\n整张图像输入，直接输出回归层 Bounding Box的位置和Bounding Box所属的类别\n\n将一幅图像分成SxS个网格(grid cell)，如果某个object的中心落在这个网格中，则这个网格就负责预测这个object。\n\n缺点：当被检测目标集中在某个区域时效果很差。因为每个格子只能对应预测一个bounding box和类别概率，因此无法预测邻近区域的物体，检测精度下降了。\n\nSSD（Single Shot Multi-box Detector）\npredict object（物体），以及其 归属类别的 score（得分）；同时在 feature map 上使用小的卷积核，去 predict 一系列 bounding boxes 的 box offsets。\n\n目标检测再进一步------行为动作捕捉。\n\n\n\n\n\n\n\n\n## 论文目录：\n\n[2013-----RCNN----Rich feature hierarchies for accurate object detection and semantic segmentation](https://zhuanlan.zhihu.com/p/29936564)\n\n2013----Deep Neural Networks for Object Detection\n\n2014----SPPnet----Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition-sppnet\n\n[2015----Fast R-CNN](https://zhuanlan.zhihu.com/p/29953111)\n\n[2016----Faster R-CNN-Towards Real-Time Object Detection with Region Proposal Networks](https://zhuanlan.zhihu.com/p/29969145)\n\n2016----Inside-Outside Net_Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n\n[2016----R-FCN-Object Detection via Region-based Fully Convolutional Networks](https://zhuanlan.zhihu.com/p/30788068)\n\n[2016----SSD-Single Shot MultiBox Detector](https://zhuanlan.zhihu.com/p/30478644)\n\n[目标检测之SSD代码分析（MXNet版）](https://zhuanlan.zhihu.com/p/30553929)\n\n2016----YOLO9000-better,faster,stronger\n\n2016----You Only Look Once-Unified, Real-Time Object Detection\n\n2017----A-Fast-RCNN_Hard Positive Generation via Adversary for Object Detection\n\n[2017----Deformable Convolutional Networks](https://zhuanlan.zhihu.com/p/30927896)\n\n2017----DSOD_ Learning Deeply Supervised Object Detectors from Scratch\n\n[2017----Focal Loss for Dense Object Detection](https://zhuanlan.zhihu.com/p/30701067)\n\n2017----Mask R-CNN\n\n2017----Speed_Accuracy trade-offs for modern convolutional object detectors\n\n[2017----Light-Head R-CNN_In Defense of Two-Stage Object Detector](https://zhuanlan.zhihu.com/p/31389174)\n\n\n\n[^1]: Nulla facilisi. Ut feugiat.\n[^2]: Nullam dui erat, malesuada eget viverra non. [malesuada](https://none)\n[^3]: Phasellus turpis odio, scelerisque eu purus ac\n",[[1525260013450,["ww@DESKTOP-CF9KSFJ",[[1,813,"![v2-19c03377416e437a288e29bd27e97c14_b]($res/v2-19c03377416e437a288e29bd27e97c14_b.jpg)\n\n"]],[812,812],[902,902]]],[1525260015859,["ww@DESKTOP-CF9KSFJ",[[-1,812,"\n"]],[812,812],[811,811]]],[1525260036338,["ww@DESKTOP-CF9KSFJ",[[1,903,"\n"]],[902,902],[903,903]]],[1525260036472,["ww@DESKTOP-CF9KSFJ",[[1,904,"\n"]],[903,903],[904,904]]],[1525260037957,["ww@DESKTOP-CF9KSFJ",[[1,902,"(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;\n\n(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。\n\n(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。\n\n就这样一直重复，找到所有被保留下来的矩形框。\n\n作者：晓雷\n链接：https://zhuanlan.zhihu.com/p/23006190\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"]],[902,902],[1186,1186]]],[1525260041624,["ww@DESKTOP-CF9KSFJ",[[-1,1099,"作者：晓雷\n链接：https://zhuanlan.zhihu.com/p/23006190\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n\n"]],[1098,1187],[1098,1098]]],[1525260042024,["ww@DESKTOP-CF9KSFJ",[[-1,1099,"\n"]],[1098,1098],[1097,1097]]],[1525260085161,["ww@DESKTOP-CF9KSFJ",[[-1,1099,"RCNN会从一张图片中找出n个可能是物体的矩形框，然后为每个矩形框为做类别分类概率："]],[1099,1141],[1099,1099]]],[1525260093898,["ww@DESKTOP-CF9KSFJ",[[-1,807,"："]],[808,808],[807,807]]],[1525260095609,["ww@DESKTOP-CF9KSFJ",[[-1,795," "]],[796,796],[795,795]]],[1525260097874,["ww@DESKTOP-CF9KSFJ",[[-1,805,"）"]],[806,806],[805,805]]],[1525260178738,["ww@DESKTOP-CF9KSFJ",[[1,3423,"\n"]],[3422,3422],[3423,3423]]],[1525260178897,["ww@DESKTOP-CF9KSFJ",[[1,3424,"\n"]],[3423,3423],[3424,3424]]],[1525260196201,["ww@DESKTOP-CF9KSFJ",[[1,3423,"http://www.rossgirshick.info/"]],[3422,3423],[3452,3452]]],[1525260214938,["ww@DESKTOP-CF9KSFJ",[[1,3423,"rgb[]"]],[3423,3423],[3428,3428]]],[1525260216490,["ww@DESKTOP-CF9KSFJ",[[1,3427,"1"]],[3427,3427],[3428,3428]]],[1525260219089,["ww@DESKTOP-CF9KSFJ",[[-1,3427,"1"]],[3428,3428],[3427,3427]]],[1525260219826,["ww@DESKTOP-CF9KSFJ",[[1,3427,"1"]],[3427,3427],[3428,3428]]],[1525260240242,["ww@DESKTOP-CF9KSFJ",[[1,3427,"^"]],[3426,3429],[3430,3430]]],[1525260243490,["ww@DESKTOP-CF9KSFJ",[[-1,3430,"http://www.rossgirshick.info/"]],[3430,3459],[3430,3430]]],[1525260258208,["ww@DESKTOP-CF9KSFJ",[[-1,8214,"Nulla facilisi. Ut feugiat."]],[8241,8241],[8214,8214]]],[1525260258960,["ww@DESKTOP-CF9KSFJ",[[1,8214,"http://www.rossgirshick.info/"]],[8214,8214],[8243,8243]]],[1525260262529,["ww@DESKTOP-CF9KSFJ",[[-1,8244,"[^2]: Nullam dui erat, malesuada eget viverra non. [malesuada](https://none)\n[^3]: Phasellus turpis odio, scelerisque eu purus ac"]],[8244,8373],[8244,8244]]]]]]}